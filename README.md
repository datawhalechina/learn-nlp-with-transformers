# transformers-quick-start-zh
we want to create a repo to illustrate usage of transformers in chinese.
# Transformerså¿«é€Ÿä¸Šæ‰‹
åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½ç®€æ´å¾—è®²è§£transformerç›¸å…³çš„æ¨¡å‹åŸç†å’Œä»£ç ä¾‹å­åŠå…¶åœ¨å„ç±»æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç›®å‰æœ¬é¡¹ç›®ä¸­çš„è®²è§£å’Œä¾‹å­ä¸»è¦åŸºäº[HuggingFace/transformers](https://github.com/huggingface/transformers)ï¼Œç”±äº[HuggingFace/transformers](https://github.com/huggingface/transformers)ä¸­è®¡ç®—æœºè§†è§‰çš„ä¾‹å­è¾ƒå°‘ï¼Œåç»­ä¼šè¿›ä¸€æ­¥æ‰©å……å…¶ä»–æµè¡Œçš„cvä»£ç åº“ã€‚

## transformersè§£å†³å“ªäº›æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Ÿ

### transformeråº”ç”¨-è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡
- ä½¿ç”¨tranformersä¸­çš„é¢„è®­ç»ƒæ¨¡å‹ç›´æ¥å¤„ç†å¯¹åº”çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚
- é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¯”å¦‚BERTï¼‰ï¼Œåœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œè·å¾—æ›´å¥½çš„æ¨¡å‹æ•ˆæœã€‚
1. [è¯­è¨€æ¨¡å‹](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/1-language_modeling-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.ipynb) Language Modeling
2. [æœºå™¨ç¿»è¯‘](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/2-translation-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.ipynb)ï¼ŒTranslation
3. [æŠ½å–å¼é—®ç­”/æœºå™¨é˜…è¯»ç†è§£](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/3-question_answering-%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94.ipynb)ï¼ŒExtractive Question Answering
4. [æ‘˜è¦ç”Ÿæˆ](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/4-summarization-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90.ipynb) summarization
5. [tokenåˆ†ç±»](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1/5-token_classification-%E8%AF%8D_%E7%AC%A6%E5%8F%B7_token%E7%BA%A7%E5%88%AB%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1.ipynb)ï¼ˆåè¯å®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ç­‰ï¼‰
6. åºåˆ—åˆ†ç±»ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå¥å­/æ–‡æœ¬æ®µè½ï¼‰ï¼ŒSequence Classification
7.  å¯¹è¯ç³»ç»Ÿï¼ŒDialogue

### transformeråº”ç”¨-è®¡ç®—æœºè§†è§‰
1. å›¾åƒåˆ†ç±»
2. å›¾åƒæ£€æµ‹
3. å›¾åƒåˆ†å‰²
### transformeråº”ç”¨-è¯­éŸ³ä¿¡å·å¤„ç†
### transformeråº”ç”¨-æ¨èç³»ç»Ÿ

## transformersçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
1. [å›¾è§£attention](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/transformer%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3/1-%E5%9B%BE%E8%A7%A3attetion.md)
2. [å›¾è§£transformer](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/transformer%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3/2-%E5%9B%BE%E8%A7%A3transformer.md)
3. [å›¾è§£BERT](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/transformer%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3/3-%E5%9B%BE%E8%A7%A3BERT.md)
4. [å›¾è§£GPT](https://github.com/datawhalechina/transformers-quick-start-zh/blob/main/transformer%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3/4-%E5%9B%BE%E8%A7%A3GPT.md)

## transformerså¦‚ä½•æ”¹è¿›ï¼Ÿ
1. çŸ¥è¯†è’¸é¦
2. æµ®ç‚¹æ•°é‡åŒ–
3. é•¿åºåˆ—transformers
4. ...
## ç¯å¢ƒé…ç½®
ç°åœ¨ä»¥macç”µè„‘ä¸ºä¾‹ï¼Œä¾æ¬¡æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

å°†æœ¬é¡¹ç›®cloneåˆ°æœ¬åœ°ï¼Œæ‰“å¼€mac/linuxçš„å‘½ä»¤è¡Œçª—å£ï¼š
```
git clone git@github.com:datawhalechina/transformers-quick-start-zh.git
```
è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹ï¼š
```
cd transformers-quick-start-zh
```
åˆ›å»ºå±äºæœ¬é¡¹ç›®çš„python3çš„ç¯å¢ƒï¼š
```
virtualenv -p python3 venv
```
æ¿€æ´»æœ¬é¡¹ç›®çš„python3ç¯å¢ƒï¼š
```
source activate venv/bin/activateï¼š
```
å®‰è£…python3.6ä¾èµ–çš„è½¯ä»¶åŒ…ï¼š
```
pip install -r requirements.txt
```

## Transformerså…·ä½“ä½¿ç”¨å’Œå®è·µï¼Ÿ


Transformeræ˜¯ä¸€ç§ç¥ç»ç½‘ç»œç»“æ„ï¼Œå®ƒå’ŒCNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰å’ŒLSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼‰ä¸€æ ·ï¼Œéƒ½èƒ½è¢«æ‹¿æ¥è§£å†³å„ç±»æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚å¦‚æœåˆå­¦è€…ä¸æ˜ç™½CNNå’ŒLSTMï¼Œåˆ™å¯ä»¥å°†transformer/cnn/lstmæƒ³åƒæˆä¸€ç§å‡½æ•°æ˜ å°„å…³ç³»ï¼Œè¿™äº›å‡½æ•°æ˜ å°„èƒ½å°†è¾“å…¥æ•°æ®æ˜ å°„æˆæˆ‘ä»¬æƒ³è¦çš„è¾“å‡ºæ•°æ®ï¼ˆæ¯”å¦‚å°†è¾“å…¥çš„ä¸€å¥è¯ï¼štransformerä¸­æ–‡å¿«é€Ÿä¸Šæ‰‹å¾ˆæ£’ï¼Œæ˜ å°„æˆè¾“å‡ºï¼š0æˆ–è€…1ã€‚è¾“å‡ºçš„0æˆ–è€…1åˆ†åˆ«ä»£è¡¨è´Ÿå‘æƒ…æ„Ÿå’Œæ­£å‘æƒ…æ„Ÿï¼‰ã€‚

åœ¨å¼€å§‹å­¦ä¹ transformerä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½çŸ¥é“ç›®å‰çš„transformerç»“æ„åŠå…¶å˜ä½“èƒ½å¸®åŠ©æˆ‘ä»¬å¾ˆå¥½çš„è§£å†³å“ªäº›æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Ÿ

![transformerså…³ç³»å›¾](./pictures/transformer_relations_cropped.jpg)
<center>å›¾1 æ·±åº¦å­¦ä¹ ä»»åŠ¡å’Œtransformerå…³ç³»å›¾</center>

å¦‚æ·±åº¦å­¦ä¹ ä»»åŠ¡å’Œtransformerå…³ç³»å›¾æ‰€ç¤ºï¼Œ**å·¦è¾¹**å±•ç¤ºäº†Huggingface/transformersä»£ç åº“èƒ½å¤„ç†çš„å„ç§æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œ**ä¸­é—´**å±•ç¤ºçš„æ˜¯å¤„ç†è¿™äº›ä»»åŠ¡çš„ç»Ÿä¸€æµæ°´çº¿ï¼ˆpipelineï¼‰ï¼Œ**å³è¾¹**å±•ç¤ºçš„æ˜¯ä¸ä»»åŠ¡å¯¹åº”çš„transformersæ¨¡å‹åç§°ï¼Œ**ä¸‹æ–¹**è¡¨ç¤ºç”¨transformersè§£å†³è¿™äº›æ·±åº¦å­¦ä¹ ä»»åŠ¡å¯ä»¥åœ¨cpuæˆ–è€…gpuä¸Šè¿›è¡Œï¼Œå¯ä»¥ä½¿ç”¨tensorflowä¹Ÿå¯ä»¥ä½¿ç”¨pytorchæ¡†æ¶ã€‚

ä¸ºäº†ä¿æŒé€»è¾‘ä¸Šçš„ä¸€è‡´æ€§å¹¶ä¸”ä¾¿äºè¯»è€…ç†è§£ï¼Œå¯¹äºä»»ä½•ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œtransformersåœ¨å¤„ç†çš„æ—¶å€™åŸºæœ¬éƒ½ä¼šéµå¾ª**ä¸­é—´**çš„æµæ°´çº¿æµç¨‹ã€‚å¦å¤–ï¼Œç”±äºHuggingFace/transformersä»£ç åº“æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å‡ºèº«ï¼ˆæœ€å¼€å§‹æ˜¯ä¸ºäº†pytorchç‰ˆæœ¬çš„BERTå‡ºç°çš„ï¼‰ï¼Œæ‰€ä»¥ç›®å‰è‡ªç„¶è¯­è¨€å¤„ç†çš„transformersæ¨¡å‹è¾ƒå¤šï¼Œè®¡ç®—æœºè§†è§‰ã€è¯­éŸ³ä¿¡å·å¤„ç†å’Œå¤šæ¨¡æ€ä¿¡å·å¤„ç†çš„æ¨¡å‹è¿˜ç›¸å¯¹è¾ƒå°‘ï¼Œæ¨èç³»ç»Ÿç›¸å…³çš„è¿˜åœ¨æœŸå¾…çœ‹åˆ°ç›¸å…³çš„å¼€æºè´¡çŒ®ã€‚ä¸è¿‡æœ¬é¡¹ç›®ç›¸ä¿¡ï¼Œé™¤äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå…¶ä»–é¢†åŸŸçš„transformersç»“æ„éƒ½ä¼šé€æ¸ç«èµ·æ¥ï¼Œä¹Ÿå°†æœ‰æ›´ä¼˜ç§€ä¼˜ç§€çš„æ¨¡å‹è¿›è¡Œå¼€æºã€‚

æˆ‘ä»¬ä»¥è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¥å­æƒ…æ„Ÿåˆ†ç±»ä¸ºä¾‹ï¼Œå¯¹transformerså¤„ç†æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„pipelineæµç¨‹è¿›è¡Œä»‹ç»ã€‚
1. æ•°æ®è¾“å…¥ã€‚
   
   æ¯”å¦‚æˆ‘ä»¬è¾“å…¥çš„æ˜¯å¥å­ï¼š
`transformerssss quick start is so good!`

2. æ•°æ®é¢„å¤„ç†ã€‚
   
   è®¡ç®—æœºä¸­çš„æ¨¡å‹è®¡ç®—éƒ½éœ€è¦åŸºäºæ•°å­—è¿›è¡Œçš„ï¼Œè¿™äº›æ¨¡å‹éƒ½æš‚æ—¶æ— æ³•ç›´æ¥ç†è§£æ–‡å­—ï¼ˆä¸­æ–‡/è‹±æ–‡éƒ½ç†è§£ä¸äº†çš„ğŸ˜‚ï¼‰ã€‚æ•°æ®é¢„å¤„ç†éƒ¨åˆ†ä¼šå°†å¥å­å¤„ç†æˆæ¨¡å‹èƒ½ç†è§£çš„åŸºæœ¬å•å…ƒåŠå…¶æ•°å­—è¡¨è¾¾ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œè¿™ç§åŸºæœ¬å•å…ƒåŠå…¶å¯¹åº”çš„æ•°å­—è¡¨è¾¾å¯¹åº”å«åštokenå’Œtoken IDã€‚å°†è‡ªç„¶è¯­è¨€è½¬æ¢æˆtokençš„è¿‡ç¨‹å«åštokenizeã€‚é‚£ä¹ˆè½¬æ¢æˆtokenå’Œtoken IDä¹‹åçš„è¾“å…¥å˜æˆäº†ä»€ä¹ˆæ ·å­ï¼Ÿ
   
   
   æˆ‘ä»¬è¿™é‡Œä½¿ç”¨ä¸€ä¸ªåå­—å«åš`distilbert-base-uncased-finetuned-sst-2-english`çš„tokenizerå°†è¾“å…¥çš„å¥å­è¿›è¡Œtokenizeã€‚
   ```
   # distilbert-base-uncased-finetuned-sst-2-englishçš„å«ä¹‰æ˜¯ï¼šä¸€ç§å«åšdistilbert-base-uncasedæ¨¡å‹åœ¨ä¸€ä¸ªå«åšsst-2çš„æ•°æ®ä¸Šé¢„è®­ç»ƒçš„transformeræ¨¡å‹
   from transformers import AutoTokenizer
   model_name = 'distilbert-base-uncased-finetuned-sst-2-english'
   tokenizer = AutoTokenizer.from_pretrained(model_name) 
   # æ ¹æ®tokenizerçš„åå­—è·å–é¢„è®­ç»ƒå¥½çš„tokenizer
   sentence = 'transformerssss quick start is so good!'
   tokens = tokenizer.tokenize(sentence, add_special_tokens=False) 
   # å¯¹å¥å­è¿›è¡Œtokenize
   print(tokens) # å¾—åˆ°çš„æ˜¯['transformers', '##ss', '##s', 'quick', 'start', 'is', 'so', 'good', '!']

   model_input = model_input = tokenizer([sentence], return_tensors="pt")
   # æ ¹æ®æ¨¡å‹è¾“å…¥è¦æ±‚è¿›è¡Œtokenizeå¹¶è½¬æ¢ä¸ºtoken id
   # return_tensors=ptä»£è¡¨è¿”å›pytorchæ ¼å¼çš„tensor
   print(model_input)
   #{'input_ids': [101, 19081, 4757, 2015, 4248, 2707, 2003, 2061, 2204, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
   ```
   ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æœ‰ç‰¹å®šçš„è¾“å…¥è¦æ±‚ï¼Œæ¯”å¦‚`distilbert-base-uncased-finetuned-sst-2-english`è¿™ä¸ªé¢„è®­ç»ƒå¥½çš„transformeræ¨¡å‹åˆ™è¦æ±‚æˆ‘ä»¬çš„åœ¨è¾“å…¥å¥å­çš„å‰åæ·»åŠ ç‰¹æ®Šç¬¦å·ï¼ˆadd_special_tokensï¼‰ï¼Œè¿™äº›ç¬¦å·å¯¹åº”çš„input_idsåˆ†åˆ«æ˜¯101å’Œ102.

   å¥å­æœ¬èº«è¢«tokenizeä¹‹åçš„å¥å­å˜æˆäº†tokensã€‚tokensä¼šå¯¹åº”ç€`['transformers', '##ss', '##s', 'quick', 'start', 'is', 'so', 'good', '!']`ï¼Œtoken IDå¯¹åº”ç€`[19081, 4757, 2015, 4248, 2707, 2003, 2061, 2204, 999]`ã€‚tokenå’Œtoken IDæ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œæœ‰äº†token IDï¼Œtransformersæ¨¡å‹ä¾¿å¯ä»¥æ ¹æ®æ­¤IDå»è·å–å±äºè¯¥IDçš„token embeddingï¼ˆä¸€ç§å‘é‡ï¼‰ã€‚

3. æ¨¡å‹è®¡ç®—ã€‚æ•°æ®é¢„å¤„ç†å¾—åˆ°äº†æ¨¡å‹è¦æ±‚æ ¼å¼çš„è¾“å…¥ä¹‹åï¼Œåˆ™å¯ä»¥è°ƒç”¨æ¨¡å‹ï¼Œå¹¶å°†å¤„ç†å¥½çš„è¾“å…¥é€å…¥æ¨¡å‹ï¼Œå¾—åˆ°è¾“å‡ºã€‚
    ```
    from transformers import AutoModelForSequenceClassification
    transformer_model = AutoModelForSequenceClassification.from_pretrained(model_name)
    # ä½¿ç”¨å’Œtokenizerç›¸åŒåå­—çš„é¢„è®­ç»ƒæ¨¡å‹
    output = transformer_model(**model_input)
    # ä½¿ç”¨transformer æ¨¡å‹å¯¹è¾“å…¥è¿›è¡Œå¤„ç†å¾—åˆ°è¾“å‡º
    print(output[0])
    # å¾—åˆ°è¾“å‡ºçš„tensor
    # tensor([[-4.0282,  4.3470]], grad_fn=<AddmmBackward>)
    import torch.nn.functional as F
    output_softmax = F.softmax(output[0], dim=-1)
    print(output_softmax)
    # tensor([[2.3045e-04, 9.9977e-01]], grad_fn=<SoftmaxBackward>)
    # ä¸Šé¢è¿™ä¸ªtensoræ˜¯2ç»´çš„ï¼Œä¸‹æ ‡åˆ†åˆ«æ˜¯0å’Œ1ï¼Œå¯¹åº”0çš„æ¦‚ç‡æ˜¯2.3045e-4,å¯¹åº”1çš„æ¦‚ç‡æ˜¯0.999977
    # è¯´æ˜1çš„æ¦‚ç‡æ›´å¤§ï¼Œè¾“å‡º1
    prediction = torch.argmax(output_softmax)
    print(prediction)
    ```
4. æ•°æ®åå¤„ç†ã€‚ç”±äºæˆ‘ä»¬æ˜¯å¯¹å¥å­è¿›è¡Œ2åˆ†ç±»ï¼Œ0ä»£è¡¨è´Ÿå‘æƒ…æ„Ÿï¼Œ1ä»£è¡¨æ­£å‘æƒ…æ„Ÿã€‚æœ¬ä»»åŠ¡è¾ƒç®€å•ï¼Œæš‚æ—¶ä¸éœ€è¦å…¶ä»–åå¤„ç†äº†ã€‚
5. è¾“å‡ºï¼šæ­£å‘æƒ…æ„Ÿã€‚

æŠŠæ•´ä¸ªè¿‡ç¨‹å…¨éƒ¨æ”¾åœ¨ä¸€èµ·å°±å«åšï¼špipelineï¼Œå¯¹åº”è¿™å›¾1ä¸­é—´çš„æµæ°´çº¿ã€‚
```
from transformers import pipeline
classifier = pipeline('sentiment-analysis', model=transformer_model, tokenizer=tokenizer)
output = classifier(sentence)
print(output)
# [{'label': 'POSITIVE', 'score': 0.9997695088386536}]
# å¾—åˆ°è¾“å‡ºæ˜¯æ­£å‘çš„æƒ…æ„Ÿã€‚
```

ä½¿ç”¨åœ¨transformersä»£ç åº“å¤„ç†å…¶ä»–æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸ä¸Šé¢çš„æµç¨‹ç›¸ä¼¼ã€‚åç»­ç« èŠ‚å°†åˆ†å¼€è¿›è¡Œä»‹ç»ã€‚

æ ¸å¿ƒæ˜¯ï¼šé€‰æ‹©é€‚åˆè‡ªå·±ä»»åŠ¡çš„transformeræ¨¡å‹ï¼Œå¯¹è¾“å…¥è¿›è¡Œé¢„å¤„ç†ï¼Œæ¨¡å‹é¢„æµ‹ï¼Œåå¤„ç†æ¨¡å‹é¢„æµ‹ï¼Œè¾“å‡ºç»“æœã€‚
# å¦‚ä½•å‚ä¸è¿›æ¥ï¼Ÿ
æˆ‘ä»¬æ¬¢è¿æ¯ä¸€ä½æœ‰å¿—äºå¼€æºè´¡çŒ®çš„åŒå­¦ä¸€èµ·æ¥å»ºè®¾transformersä¸­æ–‡ç¤¾åŒºï¼ŒæœŸæœ›å¯ä»¥å¸®åŠ©åˆ°æ›´å¤šçš„æ–°æ‰‹å¿«é€Ÿå…¥é—¨ï¼







