æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨[ç¯‡ç« 4ä»£ç åº“ä¸­](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)ã€‚

å¦‚æœæ‚¨æ­£åœ¨googleçš„colabä¸­æ‰“å¼€è¿™ä¸ªnotebookï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…Transformerså’ŒğŸ¤—Datasetsåº“ã€‚å°†ä»¥ä¸‹å‘½ä»¤å–æ¶ˆæ³¨é‡Šå³å¯å®‰è£…ã€‚


```python
!pip install datasets transformers seqeval
```

å¦‚æœæ‚¨æ­£åœ¨æœ¬åœ°æ‰“å¼€è¿™ä¸ªnotebookï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»è¿›è¡Œä¸Šè¿°ä¾èµ–åŒ…çš„å®‰è£…ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/master/examples/token-classification)æ‰¾åˆ°æœ¬notebookçš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç‰ˆæœ¬ã€‚

æœ¬å°èŠ‚æ‰€æ¶‰åŠçš„æ¨¡å‹ç»“æ„ä¸ä¸Šä¸€ç¯‡ç« ä¸­çš„BERTåŸºæœ¬ä¸€è‡´ï¼Œé¢å¤–éœ€è¦å­¦ä¹ çš„æ˜¯ç‰¹å®šä»»åŠ¡çš„æ•°æ®å¤„ç†æ–¹æ³•å’Œæ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚

# *åºåˆ—æ ‡æ³¨ï¼ˆtokençº§çš„åˆ†ç±»é—®é¢˜ï¼‰*

åºåˆ—æ ‡æ³¨ï¼Œé€šå¸¸ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯tokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ï¼šå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»ã€‚åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨[ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä¸­çš„transformeræ¨¡å‹å»åštokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ã€‚tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡é€šå¸¸æŒ‡çš„æ˜¯ä¸ºä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸€ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ ‡ç­¾ç»“æœã€‚ä¸‹å›¾å±•ç¤ºçš„æ˜¯ä¸€ä¸ªNERå®ä½“åè¯è¯†åˆ«ä»»åŠ¡ã€‚

![Widget inference representing the NER task](https://github.com/huggingface/notebooks/blob/master/examples/images/token_classification.png?raw=1)

æœ€å¸¸è§çš„tokençº§åˆ«åˆ†ç±»ä»»åŠ¡:

- NER (Named-entity recognition åè¯-å®ä½“è¯†åˆ«) åˆ†è¾¨å‡ºæ–‡æœ¬ä¸­çš„åè¯å’Œå®ä½“ (personäººå, organizationç»„ç»‡æœºæ„å, locationåœ°ç‚¹å...).
- POS (Part-of-speech taggingè¯æ€§æ ‡æ³¨) æ ¹æ®è¯­æ³•å¯¹tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ (nounåè¯, verbåŠ¨è¯, adjectiveå½¢å®¹è¯...)
- Chunk (ChunkingçŸ­è¯­ç»„å—) å°†åŒä¸€ä¸ªçŸ­è¯­çš„tokensç»„å—æ”¾åœ¨ä¸€èµ·ã€‚

å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

åªè¦é¢„è®­ç»ƒçš„transformeræ¨¡å‹æœ€é¡¶å±‚æœ‰ä¸€ä¸ªtokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œå±‚ï¼ˆæ¯”å¦‚ä¸Šä¸€ç¯‡ç« æåˆ°çš„`BertForTokenClassification`ï¼‰ï¼ˆå¦å¤–ï¼Œç”±äºtransformeråº“çš„tokenizeræ–°ç‰¹æ€§ï¼Œå¯èƒ½è¿˜éœ€è¦å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æœ‰fast tokenizerè¿™ä¸ªåŠŸèƒ½ï¼Œå‚è€ƒ[è¿™ä¸ªè¡¨](https://huggingface.co/transformers/index.html#bigtable)ï¼‰ï¼Œé‚£ä¹ˆæœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ[æ¨¡å‹é¢æ¿](https://huggingface.co/models)ï¼‰ï¼Œè§£å†³ä»»ä½•tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ã€‚

å¦‚æœæ‚¨æ‰€å¤„ç†çš„ä»»åŠ¡æœ‰æ‰€ä¸åŒï¼Œå¤§æ¦‚ç‡åªéœ€è¦å¾ˆå°çš„æ”¹åŠ¨ä¾¿å¯ä»¥ä½¿ç”¨æœ¬notebookè¿›è¡Œå¤„ç†ã€‚åŒæ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨çš„GPUæ˜¾å­˜æ¥è°ƒæ•´å¾®è°ƒè®­ç»ƒæ‰€éœ€è¦çš„btach sizeå¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚



```python
task = "ner" #éœ€è¦æ˜¯"ner", "pos" æˆ–è€… "chunk"
model_checkpoint = "distilbert-base-uncased"
batch_size = 16
```

## åŠ è½½æ•°æ®


æˆ‘ä»¬å°†ä¼šä½¿ç”¨[ğŸ¤— Datasets](https://github.com/huggingface/datasets)åº“æ¥åŠ è½½æ•°æ®å’Œå¯¹åº”çš„è¯„æµ‹æ–¹å¼ã€‚æ•°æ®åŠ è½½å’Œè¯„æµ‹æ–¹å¼åŠ è½½åªéœ€è¦ç®€å•ä½¿ç”¨`load_dataset`å’Œ`load_metric`å³å¯ã€‚


```python
from datasets import load_dataset, load_metric
```

æœ¬notebookä¸­çš„ä¾‹å­ä½¿ç”¨çš„æ˜¯[CONLL 2003 dataset](https://www.aclweb.org/anthology/W03-0419.pdf)æ•°æ®é›†ã€‚è¿™ä¸ªnotebookåº”è¯¥å¯ä»¥å¤„ç†ğŸ¤— Datasetsåº“ä¸­çš„ä»»ä½•tokenåˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æ‚¨è‡ªå®šä¹‰çš„json/csvæ–‡ä»¶æ•°æ®é›†ï¼Œæ‚¨éœ€è¦æŸ¥çœ‹[æ•°æ®é›†æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)æ¥å­¦ä¹ å¦‚ä½•åŠ è½½ã€‚è‡ªå®šä¹‰æ•°æ®é›†å¯èƒ½éœ€è¦åœ¨åŠ è½½å±æ€§åå­—ä¸Šåšä¸€äº›è°ƒæ•´ã€‚


```python
datasets = load_dataset("conll2003")
```

è¿™ä¸ª`datasets`å¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)æ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚


```python
datasets
```




    DatasetDict({
        train: Dataset({
            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
            num_rows: 14041
        })
        validation: Dataset({
            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
            num_rows: 3250
        })
        test: Dataset({
            features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
            num_rows: 3453
        })
    })



æ— è®ºæ˜¯åœ¨è®­ç»ƒé›†ã€éªŒè¯æœºè¿˜æ˜¯æµ‹è¯•é›†ä¸­ï¼Œdatasetséƒ½åŒ…å«äº†ä¸€ä¸ªåä¸ºtokensçš„åˆ—ï¼ˆä¸€èˆ¬æ¥è¯´æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆäº†å¾ˆå¤šè¯ï¼‰ï¼Œè¿˜åŒ…å«ä¸€ä¸ªåä¸ºlabelçš„åˆ—ï¼Œè¿™ä¸€åˆ—å¯¹åº”è¿™tokensçš„æ ‡æ³¨ã€‚

ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚


```python
datasets["train"][0]
```




    {'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],
     'id': '0',
     'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],
     'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],
     'tokens': ['EU',
      'rejects',
      'German',
      'call',
      'to',
      'boycott',
      'British',
      'lamb',
      '.']}



æ‰€æœ‰çš„æ•°æ®æ ‡ç­¾labelséƒ½å·²ç»è¢«ç¼–ç æˆäº†æ•´æ•°ï¼Œå¯ä»¥ç›´æ¥è¢«é¢„è®­ç»ƒtransformeræ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›æ•´æ•°çš„ç¼–ç æ‰€å¯¹åº”çš„å®é™…ç±»åˆ«å‚¨å­˜åœ¨`features`ä¸­ã€‚


```python
datasets["train"].features[f"ner_tags"]
```




    Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)



æ‰€ä»¥ä»¥NERä¸ºä¾‹ï¼Œ0å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«æ˜¯â€Oâ€œï¼Œ 1å¯¹åº”çš„æ˜¯â€B-PERâ€œç­‰ç­‰ã€‚â€Oâ€œçš„æ„æ€æ˜¯æ²¡æœ‰ç‰¹åˆ«å®ä½“ï¼ˆno special entityï¼‰ã€‚æœ¬ä¾‹åŒ…å«4ç§å®ä½“ç±»åˆ«åˆ†åˆ«æ˜¯ï¼ˆPERã€ORGã€LOCï¼ŒMISCï¼‰ï¼Œæ¯ä¸€ç§å®ä½“ç±»åˆ«åˆåˆ†åˆ«æœ‰B-ï¼ˆå®ä½“å¼€å§‹çš„tokenï¼‰å‰ç¼€å’ŒI-ï¼ˆå®ä½“ä¸­é—´çš„tokenï¼‰å‰ç¼€ã€‚

- 'PER' for person
- 'ORG' for organization
- 'LOC' for location
- 'MISC' for miscellaneous


```python
label_list = datasets["train"].features[f"{task}_tags"].feature.names
label_list
```




    ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']



ä¸ºäº†èƒ½å¤Ÿè¿›ä¸€æ­¥ç†è§£æ•°æ®é•¿ä»€ä¹ˆæ ·å­ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºã€‚


```python
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>tokens</th>
      <th>pos_tags</th>
      <th>chunk_tags</th>
      <th>ner_tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2227</td>
      <td>[Result, of, a, French, first, division, match, on, Friday, .]</td>
      <td>[NN, IN, DT, JJ, JJ, NN, NN, IN, NNP, .]</td>
      <td>[B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, I-NP, B-PP, B-NP, O]</td>
      <td>[O, O, O, B-MISC, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2615</td>
      <td>[Mid-tier, golds, up, in, heavy, trading, .]</td>
      <td>[NN, NNS, IN, IN, JJ, NN, .]</td>
      <td>[B-NP, I-NP, B-PP, B-PP, B-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10256</td>
      <td>[Neagle, (, 14-6, ), beat, the, Braves, for, the, third, time, this, season, ,, allowing, two, runs, and, six, hits, in, eight, innings, .]</td>
      <td>[NNP, (, CD, ), VB, DT, NNPS, IN, DT, JJ, NN, DT, NN, ,, VBG, CD, NNS, CC, CD, NNS, IN, CD, NN, .]</td>
      <td>[B-NP, O, B-NP, O, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, I-NP, O, B-NP, I-NP, B-PP, B-NP, I-NP, O]</td>
      <td>[B-PER, O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10720</td>
      <td>[Hansa, Rostock, 4, 1, 2, 1, 5, 4, 5]</td>
      <td>[NNP, NNP, CD, CD, CD, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7125</td>
      <td>[MONTREAL, 70, 59, .543, 11]</td>
      <td>[NNP, CD, CD, CD, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, O, O, O]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3316</td>
      <td>[Softbank, Corp, said, on, Friday, that, it, would, procure, $, 900, million, through, the, foreign, exchange, market, by, September, 5, as, part, of, its, acquisition, of, U.S., firm, ,, Kingston, Technology, Co, .]</td>
      <td>[NNP, NNP, VBD, IN, NNP, IN, PRP, MD, NN, $, CD, CD, IN, DT, JJ, NN, NN, IN, NNP, CD, IN, NN, IN, PRP$, NN, IN, NNP, NN, ,, NNP, NNP, NNP, .]</td>
      <td>[B-NP, I-NP, B-VP, B-PP, B-NP, B-SBAR, B-NP, B-VP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, O, B-NP, I-NP, I-NP, O]</td>
      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, B-ORG, I-ORG, I-ORG, O]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3923</td>
      <td>[Ghent, 3, Aalst, 2]</td>
      <td>[NN, CD, NNP, CD]</td>
      <td>[B-NP, I-NP, I-NP, I-NP]</td>
      <td>[B-ORG, O, B-ORG, O]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2776</td>
      <td>[The, separatists, ,, who, swept, into, Grozny, on, August, 6, ,, still, control, large, areas, of, the, centre, of, town, ,, and, Russian, soldiers, are, based, at, checkpoints, on, the, approach, roads, .]</td>
      <td>[DT, NNS, ,, WP, VBD, IN, NNP, IN, NNP, CD, ,, RB, VBP, JJ, NNS, IN, DT, NN, IN, NN, ,, CC, JJ, NNS, VBP, VBN, IN, NNS, IN, DT, NN, NNS, .]</td>
      <td>[B-NP, I-NP, O, B-NP, B-VP, B-PP, B-NP, B-PP, B-NP, I-NP, O, B-ADVP, B-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O, O, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, O]</td>
      <td>[O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1178</td>
      <td>[Doctor, Masserigne, Ndiaye, said, medical, staff, were, overwhelmed, with, work, ., "]</td>
      <td>[NNP, NNP, NNP, VBD, JJ, NN, VBD, VBN, IN, NN, ., "]</td>
      <td>[B-NP, I-NP, I-NP, B-VP, B-NP, I-NP, B-VP, I-VP, B-PP, B-NP, O, O]</td>
      <td>[O, B-PER, I-PER, O, O, O, O, O, O, O, O, O]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10988</td>
      <td>[Reuters, historical, calendar, -, September, 4, .]</td>
      <td>[NNP, JJ, NN, :, NNP, CD, .]</td>
      <td>[B-NP, I-NP, I-NP, O, B-NP, I-NP, O]</td>
      <td>[B-ORG, O, O, O, O, O, O]</td>
    </tr>
  </tbody>
</table>


## é¢„å¤„ç†æ•°æ®

åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«`Tokenizer`ã€‚`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨`AutoTokenizer.from_pretrained`æ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š

- æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚
- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚

è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

æ³¨æ„ï¼šä»¥ä¸‹ä»£ç è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚

å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerã€‚æˆ‘ä»¬å¯ä»¥åœ¨[æ¨¡å‹tokenizerå¯¹åº”è¡¨](https://huggingface.co/transformers/index.html#bigtable)é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚


```python
import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)
```

åœ¨[è¿™é‡Œbig table of models](https://huggingface.co/transformers/index.html#bigtable)æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æœ‰fast tokenizerã€‚

tokenizeræ—¢å¯ä»¥å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸€å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œtokenizeré¢„å¤„ç†åå¾—åˆ°çš„æ•°æ®æ»¡è¶³é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼


```python
tokenizer("Hello, this is one sentence!")
```




    {'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}






```python
tokenizer(["Hello", ",", "this", "is", "one", "sentence", "split", "into", "words", "."], is_split_into_words=True)
```




    {'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}



æ³¨æ„transformeré¢„è®­ç»ƒæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™é€šå¸¸ä½¿ç”¨çš„æ˜¯subwordï¼Œå¦‚æœæˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥å·²ç»è¢«åˆ‡åˆ†æˆäº†wordï¼Œé‚£ä¹ˆè¿™äº›wordè¿˜ä¼šè¢«æˆ‘ä»¬çš„tokenizerç»§ç»­åˆ‡åˆ†ã€‚ä¸¾ä¸ªä¾‹å­ï¼š



```python
example = datasets["train"][4]
print(example["tokens"])
```

    ['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']



```python
tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
print(tokens)
```

    ['[CLS]', 'germany', "'", 's', 'representative', 'to', 'the', 'european', 'union', "'", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']


å•è¯"Zwingmann" å’Œ "sheepmeat"ç»§ç»­è¢«åˆ‡åˆ†æˆäº†3ä¸ªsubtokensã€‚

ç”±äºæ ‡æ³¨æ•°æ®é€šå¸¸æ˜¯åœ¨wordçº§åˆ«è¿›è¡Œæ ‡æ³¨çš„ï¼Œæ—¢ç„¶wordè¿˜ä¼šè¢«åˆ‡åˆ†æˆsubtokensï¼Œé‚£ä¹ˆæ„å‘³ç€æˆ‘ä»¬è¿˜éœ€è¦å¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œsubtokensçš„å¯¹é½ã€‚åŒæ—¶ï¼Œç”±äºé¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼çš„è¦æ±‚ï¼Œå¾€å¾€è¿˜éœ€è¦åŠ ä¸Šä¸€äº›ç‰¹æ®Šç¬¦å·æ¯”å¦‚ï¼š `[CLS]` å’Œ  `[SEP]`ã€‚


```python
len(example[f"{task}_tags"]), len(tokenized_input["input_ids"])
```




    (31, 39)



tokenizeræœ‰ä¸€ä¸ª` `word_ids`æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚


```python
print(tokenized_input.word_ids())
```

    [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]



æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œword_idså°†æ¯ä¸€ä¸ªsubtokensä½ç½®éƒ½å¯¹åº”äº†ä¸€ä¸ªwordçš„ä¸‹æ ‡ã€‚æ¯”å¦‚ç¬¬1ä¸ªä½ç½®å¯¹åº”ç¬¬0ä¸ªwordï¼Œç„¶åç¬¬2ã€3ä¸ªä½ç½®å¯¹åº”ç¬¬1ä¸ªwordã€‚ç‰¹æ®Šå­—ç¬¦å¯¹åº”äº†Noneã€‚æœ‰äº†è¿™ä¸ªlistï¼Œæˆ‘ä»¬å°±èƒ½å°†subtokenså’Œwordsè¿˜æœ‰æ ‡æ³¨çš„labelså¯¹é½å•¦ã€‚


```python
word_ids = tokenized_input.word_ids()
aligned_labels = [-100 if i is None else example[f"{task}_tags"][i] for i in word_ids]
print(len(aligned_labels), len(tokenized_input["input_ids"]))
```

    39 39


æˆ‘ä»¬é€šå¸¸å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100ï¼Œåœ¨æ¨¡å‹ä¸­-100é€šå¸¸ä¼šè¢«å¿½ç•¥æ‰ä¸è®¡ç®—lossã€‚

æˆ‘ä»¬æœ‰ä¸¤ç§å¯¹é½labelçš„æ–¹å¼ï¼š
- å¤šä¸ªsubtokenså¯¹é½ä¸€ä¸ªwordï¼Œå¯¹é½ä¸€ä¸ªlabel
- å¤šä¸ªsubtokensçš„ç¬¬ä¸€ä¸ªsubtokenå¯¹é½wordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼Œå…¶ä»–subtokensç›´æ¥èµ‹äºˆ-100.

æˆ‘ä»¬æä¾›è¿™ä¸¤ç§æ–¹å¼ï¼Œé€šè¿‡`label_all_tokens = True`åˆ‡æ¢ã€‚



```python
label_all_tokens = True
```


æœ€åæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹åˆèµ·æ¥å˜æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°ã€‚`is_split_into_words=True`åœ¨ä¸Šé¢å·²ç»ç»“æŸå•¦ã€‚


```python
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```

ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚


```python
tokenize_and_align_labels(datasets['train'][:5])
```




    {'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}



æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚




```python
tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)
```

æ›´å¥½çš„æ˜¯ï¼Œè¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨`load_from_cache_file=False`å‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„`batched=True`è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œå› ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚

## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹


æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForTokenClassification` è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œ`from_pretrained`æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚


```python
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))
```


    Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]


    Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
    - This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯tokenåˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†tokenåˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚



ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Trainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦3ä¸ªè¦ç´ ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚


```python
args = TrainingArguments(
    f"test-{task}",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
)
```

ä¸Šé¢evaluation_strategy = "epoch"å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚

ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚

æœ€åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†æˆ‘ä»¬å¤„ç†å¥½çš„è¾“å…¥å–‚ç»™æ¨¡å‹ã€‚


```python
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)
```


è®¾ç½®å¥½`Trainer`è¿˜å‰©æœ€åä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦å®šä¹‰å¥½è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨[`seqeval`](https://github.com/chakki-works/seqeval) metricæ¥å®Œæˆè¯„ä¼°ã€‚å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåšä¸€äº›æ•°æ®åå¤„ç†ï¼š



```python
metric = load_metric("seqeval")
```

è¯„ä¼°çš„è¾“å…¥æ˜¯é¢„æµ‹å’Œlabelçš„list


```python
labels = [label_list[i] for i in example[f"{task}_tags"]]
metric.compute(predictions=[labels], references=[labels])
```




    {'LOC': {'f1': 1.0, 'number': 2, 'precision': 1.0, 'recall': 1.0},
     'ORG': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
     'PER': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
     'overall_accuracy': 1.0,
     'overall_f1': 1.0,
     'overall_precision': 1.0,
     'overall_recall': 1.0}



å¯¹æ¨¡å‹é¢„æµ‹ç»“æœåšä¸€äº›åå¤„ç†ï¼š
- é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡
- å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabel
- å¿½ç•¥-100æ‰€åœ¨åœ°æ–¹

ä¸‹é¢çš„å‡½æ•°å°†ä¸Šé¢çš„æ­¥éª¤åˆå¹¶äº†èµ·æ¥ã€‚


```python
import numpy as np

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
```

æˆ‘ä»¬è®¡ç®—æ‰€æœ‰ç±»åˆ«æ€»çš„precision/recall/f1ï¼Œæ‰€ä»¥ä¼šæ‰”æ‰å•ä¸ªç±»åˆ«çš„precision/recall/f1 

å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯



```python
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒ


```python
trainer.train()
```



    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
        </style>

      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [2634/2634 01:45, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.237721</td>
      <td>0.068198</td>
      <td>0.903148</td>
      <td>0.921132</td>
      <td>0.912051</td>
      <td>0.979713</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.053160</td>
      <td>0.059337</td>
      <td>0.927697</td>
      <td>0.932990</td>
      <td>0.930336</td>
      <td>0.983113</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.029850</td>
      <td>0.059346</td>
      <td>0.929267</td>
      <td>0.939143</td>
      <td>0.934179</td>
      <td>0.984257</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=2634, training_loss=0.08569671253227518)



æˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨`evaluate`æ–¹æ³•è¯„ä¼°ï¼Œå¯ä»¥è¯„ä¼°å…¶ä»–æ•°æ®é›†ã€‚


```python
trainer.evaluate()
```



<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
    </style>

  <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [204/204 00:05]
</div>






    {'eval_loss': 0.05934586375951767,
     'eval_precision': 0.9292672127518264,
     'eval_recall': 0.9391430808815304,
     'eval_f1': 0.9341790463472988,
     'eval_accuracy': 0.9842565968195466,
     'epoch': 3.0}



å¦‚æœæƒ³è¦å¾—åˆ°å•ä¸ªç±»åˆ«çš„precision/recall/f1ï¼Œæˆ‘ä»¬ç›´æ¥å°†ç»“æœè¾“å…¥ç›¸åŒçš„è¯„ä¼°å‡½æ•°å³å¯ï¼š


```python
predictions, labels, _ = trainer.predict(tokenized_datasets["validation"])
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
results
```




    {'LOC': {'precision': 0.949718574108818,
      'recall': 0.966768525592055,
      'f1': 0.9581677077418134,
      'number': 2618},
     'MISC': {'precision': 0.8132387706855791,
      'recall': 0.8383428107229894,
      'f1': 0.8255999999999999,
      'number': 1231},
     'ORG': {'precision': 0.9055232558139535,
      'recall': 0.9090466926070039,
      'f1': 0.9072815533980583,
      'number': 2056},
     'PER': {'precision': 0.9759552042160737,
      'recall': 0.9765985497692815,
      'f1': 0.9762767710049424,
      'number': 3034},
     'overall_precision': 0.9292672127518264,
     'overall_recall': 0.9391430808815304,
     'overall_f1': 0.9341790463472988,
     'overall_accuracy': 0.9842565968195466}



æœ€ååˆ«å¿˜äº†ï¼Œä¸Šä¼ æ¨¡å‹åˆ°[ğŸ¤— Model Hub](https://huggingface.co/models)ï¼ˆç‚¹å‡»[è¿™é‡Œ](https://huggingface.co/transformers/model_sharing.html)æ¥æŸ¥çœ‹å¦‚ä½•ä¸Šä¼ ï¼‰ã€‚éšåæ‚¨å°±å¯ä»¥åƒè¿™ä¸ªnotebookä¸€å¼€å§‹ä¸€æ ·ï¼Œç›´æ¥ç”¨æ¨¡å‹åå­—å°±èƒ½ä½¿ç”¨æ‚¨è‡ªå·±ä¸Šä¼ çš„æ¨¡å‹å•¦ã€‚



```python

```
