æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨[ç¯‡ç« 4ä»£ç åº“ä¸­](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)ã€‚

å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªjupyterç¬”è®°æœ¬ï¼Œæ‚¨éœ€è¦å®‰è£…ğŸ¤—Trasnformerså’ŒğŸ¤—datasetsã€‚å…·ä½“å‘½ä»¤å¦‚ä¸‹ï¼ˆå–æ¶ˆæ³¨é‡Šå¹¶è¿è¡Œï¼Œå¦‚æœé€Ÿåº¦æ…¢è¯·åˆ‡æ¢å›½å†…æºï¼ŒåŠ ä¸Šç¬¬äºŒè¡Œçš„å‚æ•°ï¼‰ã€‚

åœ¨è¿è¡Œå•å…ƒæ ¼ä¹‹å‰ï¼Œå»ºè®®æ‚¨æŒ‰ç…§æœ¬é¡¹ç›®readmeä¸­æç¤ºï¼Œå»ºç«‹ä¸€ä¸ªä¸“é—¨çš„pythonç¯å¢ƒç”¨äºå­¦ä¹ ã€‚


```python
#! pip install datasets transformers 
# -i https://pypi.tuna.tsinghua.edu.cn/simple
```

å¦‚æœæ‚¨æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰“å¼€è¿™ä¸ªjupyterç¬”è®°æœ¬ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå®‰è£…äº†ä¸Šè¿°åº“çš„æœ€æ–°ç‰ˆæœ¬ã€‚

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/blob/master/examples/pytorch/multiple-choice/)æ‰¾åˆ°è¿™ä¸ªjupyterç¬”è®°æœ¬çš„å…·ä½“çš„pythonè„šæœ¬æ–‡ä»¶ï¼Œè¿˜å¯ä»¥é€šè¿‡åˆ†å¸ƒå¼çš„æ–¹å¼ä½¿ç”¨å¤šä¸ªgpuæˆ–tpuæ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

# é€šè¿‡å¾®è°ƒæ¨¡å‹æ„å»ºå¤šé€‰ä»»åŠ¡

åœ¨å½“å‰jupyterç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•é€šè¿‡å¾®è°ƒä»»æ„[ğŸ¤—Transformers](https://github.com/huggingface/transformers) æ¨¡å‹æ¥æ„å»ºå¤šé€‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ˜¯åœ¨ç»™å®šçš„å¤šä¸ªç­”æ¡ˆä¸­é€‰æ‹©æœ€åˆç†çš„ä¸€ä¸ªã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯[SWAG](https://www.aclweb.org/anthology/D18-1009/)ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥å°†é¢„å¤„ç†è¿‡ç¨‹ç”¨äºå…¶ä»–å¤šé€‰æ•°æ®é›†æˆ–è€…ä½ è‡ªå·±çš„æ•°æ®ã€‚SWAGæ˜¯ä¸€ä¸ªå…³äºå¸¸è¯†æ¨ç†çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬æè¿°ä¸€ç§æƒ…å†µï¼Œç„¶åç»™å‡ºå››ä¸ªå¯èƒ½çš„é€‰é¡¹ã€‚

è¿™ä¸ªjupyterç¬”è®°æœ¬å¯ä»¥è¿è¡Œåœ¨[model Hub](https://huggingface.co/models)ä¸­çš„ä»»ä½•æ¨¡å‹ä¸Šï¼Œåªè¦è¯¥æ¨¡å‹å…·æœ‰ä¸€ä¸ªå¤šé€‰æ‹©å¤´çš„ç‰ˆæœ¬ã€‚æ ¹æ®ä½ çš„æ¨¡å‹å’Œä½ ä½¿ç”¨çš„GPUï¼Œä½ å¯èƒ½éœ€è¦è°ƒæ•´æ‰¹å¤§å°ï¼Œä»¥é¿å…æ˜¾å­˜ä¸è¶³çš„é”™è¯¯ã€‚è®¾ç½®å¥½è¿™ä¸¤ä¸ªå‚æ•°ä¹‹åï¼Œjupyterç¬”è®°æœ¬çš„å…¶ä½™éƒ¨åˆ†å°±å¯ä»¥é¡ºåˆ©è¿è¡Œäº†:


```python
model_checkpoint = "bert-base-uncased"
batch_size = 16
```

## åŠ è½½æ•°æ®é›†

æˆ‘ä»¬å°†ä½¿ç”¨[ğŸ¤—Datasets](https://github.com/huggingface/datasets)åº“æ¥ä¸‹è½½æ•°æ®ã€‚è¿™ä¸€è¿‡ç¨‹å¯ä»¥å¾ˆå®¹æ˜“åœ°ç”¨å‡½æ•°`load_dataset`æ¥å®Œæˆã€‚


```python
from datasets import load_dataset, load_metric
```

`load_dataset` å°†ç¼“å­˜æ•°æ®é›†ä»¥é¿å…ä¸‹æ¬¡è¿è¡Œæ—¶å†æ¬¡ä¸‹è½½å®ƒã€‚


```python
datasets = load_dataset("swag", "regular")
```

    Reusing dataset swag (/home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d)


é™¤æ­¤ä¹‹å¤–ï¼Œä½ ä¹Ÿå¯ä»¥ä»æˆ‘ä»¬æä¾›çš„[é“¾æ¥](https://gas.graviti.cn/dataset/datawhale/SWAG
)ä¸‹è½½æ•°æ®å¹¶è§£å‹ï¼Œå°†è§£å‹åçš„3ä¸ªcsvæ–‡ä»¶å¤åˆ¶åˆ°åˆ°`docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/datasets/swag`ç›®å½•ä¸‹ï¼Œç„¶åç”¨ä¸‹é¢çš„ä»£ç è¿›è¡ŒåŠ è½½ã€‚


```python
import os

data_path = './datasets/swag/'
cache_dir = os.path.join(data_path, 'cache')
data_files = {'train': os.path.join(data_path, 'train.csv'), 'val': os.path.join(data_path, 'val.csv'), 'test': os.path.join(data_path, 'test.csv')}
datasets = load_dataset(data_path, 'regular', data_files=data_files, cache_dir=cache_dir)
```

    Using custom data configuration regular-2ab2d66f12115abf


    Downloading and preparing dataset swag/regular (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./datasets/swag/cache/swag/regular-2ab2d66f12115abf/0.0.0/a16ae67faa24f4cdd6d1fc6bfc09bdb6dc15771716221ff8bacbc6cc75533614...


                                             

    Dataset swag downloaded and prepared to ./datasets/swag/cache/swag/regular-2ab2d66f12115abf/0.0.0/a16ae67faa24f4cdd6d1fc6bfc09bdb6dc15771716221ff8bacbc6cc75533614. Subsequent calls will reuse this data.


    

`dataset`å¯¹è±¡æœ¬èº«æ˜¯[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)ï¼Œå®ƒåŒ…å«ç”¨äºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„é”®å€¼å¯¹(`mnli`æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ä¾‹å­ï¼Œå…¶ä¸­åŒ…å«ç”¨äºä¸åŒ¹é…çš„éªŒè¯å’Œæµ‹è¯•é›†çš„é”®å€¼å¯¹)ã€‚


```python
datasets
```




    DatasetDict({
        train: Dataset({
            features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
            num_rows: 73546
        })
        validation: Dataset({
            features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
            num_rows: 20006
        })
        test: Dataset({
            features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],
            num_rows: 20005
        })
    })



To access an actual element, you need to select a split first, then give an index:


```python
datasets["train"][0]
```




    {'ending0': 'passes by walking down the street playing their instruments.',
     'ending1': 'has heard approaching them.',
     'ending2': "arrives and they're outside dancing and asleep.",
     'ending3': 'turns the lead singer watches the performance.',
     'fold-ind': '3416',
     'gold-source': 'gold',
     'label': 0,
     'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',
     'sent2': 'A drum line',
     'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',
     'video-id': 'anetv_jkn6uvmqwh4'}



ä¸ºäº†äº†è§£æ•°æ®æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Œä¸‹é¢çš„å‡½æ•°å°†æ˜¾ç¤ºæ•°æ®é›†ä¸­éšæœºé€‰å–çš„ä¸€äº›ç¤ºä¾‹ã€‚


```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ending0</th>
      <th>ending1</th>
      <th>ending2</th>
      <th>ending3</th>
      <th>fold-ind</th>
      <th>gold-source</th>
      <th>label</th>
      <th>sent1</th>
      <th>sent2</th>
      <th>startphrase</th>
      <th>video-id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>are seated on a field.</td>
      <td>are skiing down the slope.</td>
      <td>are in a lift.</td>
      <td>are pouring out in a man.</td>
      <td>16668</td>
      <td>gold</td>
      <td>1</td>
      <td>A man is wiping the skiboard.</td>
      <td>Group of people</td>
      <td>A man is wiping the skiboard. Group of people</td>
      <td>anetv_JmL6BiuXr_g</td>
    </tr>
    <tr>
      <th>1</th>
      <td>performs stunts inside a gym.</td>
      <td>shows several shopping in the water.</td>
      <td>continues his skateboard while talking.</td>
      <td>is putting a black bike close.</td>
      <td>11424</td>
      <td>gold</td>
      <td>0</td>
      <td>The credits of the video are shown.</td>
      <td>A lady</td>
      <td>The credits of the video are shown. A lady</td>
      <td>anetv_dWyE0o2NetQ</td>
    </tr>
    <tr>
      <th>2</th>
      <td>is emerging into the hospital.</td>
      <td>are strewn under water at some wreckage.</td>
      <td>tosses the wand together and saunters into the marketplace.</td>
      <td>swats him upside down.</td>
      <td>15023</td>
      <td>gen</td>
      <td>1</td>
      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave.</td>
      <td>Someone</td>
      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave. Someone</td>
      <td>lsmdc3016_CHASING_MAVERICKS-6791</td>
    </tr>
    <tr>
      <th>3</th>
      <td>spies someone sitting below.</td>
      <td>opens the fridge and checks out the photo.</td>
      <td>puts a little sheepishly.</td>
      <td>staggers up to him.</td>
      <td>5475</td>
      <td>gold</td>
      <td>3</td>
      <td>He tips it upside down, and its little umbrella falls to the floor.</td>
      <td>Back inside, someone</td>
      <td>He tips it upside down, and its little umbrella falls to the floor. Back inside, someone</td>
      <td>lsmdc1008_Spider-Man2-75503</td>
    </tr>
    <tr>
      <th>4</th>
      <td>carries her to the grave.</td>
      <td>laughs as someone styles her hair.</td>
      <td>sets down his glass.</td>
      <td>stares after her then trudges back up into the street.</td>
      <td>6904</td>
      <td>gen</td>
      <td>1</td>
      <td>Someone kisses her smiling daughter on the cheek and beams back at the camera.</td>
      <td>Someone</td>
      <td>Someone kisses her smiling daughter on the cheek and beams back at the camera. Someone</td>
      <td>lsmdc1028_No_Reservations-83242</td>
    </tr>
    <tr>
      <th>5</th>
      <td>stops someone and sweeps all the way back from the lower deck to join them.</td>
      <td>is being dragged towards the monstrous animation.</td>
      <td>beats out many events at the touch of the sword, crawling it.</td>
      <td>reaches into a pocket and yanks open the door.</td>
      <td>14089</td>
      <td>gen</td>
      <td>1</td>
      <td>But before he can use his wand, he accidentally rams it up the troll's nostril.</td>
      <td>The angry troll</td>
      <td>But before he can use his wand, he accidentally rams it up the troll's nostril. The angry troll</td>
      <td>lsmdc1053_Harry_Potter_and_the_philosophers_stone-95867</td>
    </tr>
    <tr>
      <th>6</th>
      <td>sees someone's name in the photo.</td>
      <td>gives a surprised look.</td>
      <td>kneels down and touches his ripped specs.</td>
      <td>spies on someone's clock.</td>
      <td>8407</td>
      <td>gen</td>
      <td>1</td>
      <td>Someone keeps his tired eyes on the road.</td>
      <td>Glancing over, he</td>
      <td>Someone keeps his tired eyes on the road. Glancing over, he</td>
      <td>lsmdc1024_Identity_Thief-82693</td>
    </tr>
    <tr>
      <th>7</th>
      <td>stops as someone speaks into the camera.</td>
      <td>notices how blue his eyes are.</td>
      <td>is flung out of the door and knocks the boy over.</td>
      <td>flies through the air, its a fireball.</td>
      <td>4523</td>
      <td>gold</td>
      <td>1</td>
      <td>Both people are knocked back a few steps from the force of the collision.</td>
      <td>She</td>
      <td>Both people are knocked back a few steps from the force of the collision. She</td>
      <td>lsmdc0043_Thelma_and_Luise-68271</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sits close to the river.</td>
      <td>have pet's supplies and pets.</td>
      <td>pops parked outside the dirt facility, sending up a car highway to catch control.</td>
      <td>displays all kinds of power tools and website.</td>
      <td>8112</td>
      <td>gold</td>
      <td>1</td>
      <td>A guy waits in the waiting room with his pet.</td>
      <td>A pet store and its van</td>
      <td>A guy waits in the waiting room with his pet. A pet store and its van</td>
      <td>anetv_9VWoQpg9wqE</td>
    </tr>
    <tr>
      <th>9</th>
      <td>the slender someone, someone turns on the light.</td>
      <td>, someone gives them to her boss then dumps some alcohol into dough.</td>
      <td>liquids from a bowl, she slams them drunk.</td>
      <td>wags his tail as someone returns to the hotel room.</td>
      <td>10867</td>
      <td>gold</td>
      <td>3</td>
      <td>Inside a convenience store, she opens a freezer case.</td>
      <td>Dolce</td>
      <td>Inside a convenience store, she opens a freezer case. Dolce</td>
      <td>lsmdc3090_YOUNG_ADULT-43871</td>
    </tr>
  </tbody>
</table>


æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå®ƒæ˜¯ç”±ç¬¬ä¸€ä¸ªå¥å­(å­—æ®µ`sent1`)å’Œç¬¬äºŒä¸ªå¥å­çš„ç®€ä»‹(å­—æ®µ`sent2`)ç»„æˆã€‚ç„¶åç»™å‡ºå››ç§å¯èƒ½çš„ç»“å°¾(å­—æ®µ`ending0`ï¼Œ `ending1`ï¼Œ `ending2`å’Œ`ending3`)ï¼Œç„¶åè®©æ¨¡å‹ä»ä¸­é€‰æ‹©æ­£ç¡®çš„ä¸€ä¸ª(ç”±å­—æ®µ`label`è¡¨ç¤º)ã€‚ä¸‹é¢çš„å‡½æ•°è®©æˆ‘ä»¬æ›´ç›´è§‚åœ°çœ‹åˆ°ä¸€ä¸ªç¤ºä¾‹:


```python
def show_one(example):
    print(f"Context: {example['sent1']}")
    print(f"  A - {example['sent2']} {example['ending0']}")
    print(f"  B - {example['sent2']} {example['ending1']}")
    print(f"  C - {example['sent2']} {example['ending2']}")
    print(f"  D - {example['sent2']} {example['ending3']}")
    print(f"\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}")
```


```python
show_one(datasets["train"][0])
```

    Context: Members of the procession walk down the street holding small horn brass instruments.
      A - A drum line passes by walking down the street playing their instruments.
      B - A drum line has heard approaching them.
      C - A drum line arrives and they're outside dancing and asleep.
      D - A drum line turns the lead singer watches the performance.
    
    Ground truth: option A



```python
show_one(datasets["train"][15])
```

    Context: Now it's someone's turn to rain blades on his opponent.
      A - Someone pats his shoulder and spins wildly.
      B - Someone lunges forward through the window.
      C - Someone falls to the ground.
      D - Someone rolls up his fast run from the water and tosses in the sky.
    
    Ground truth: option C


## æ•°æ®é¢„å¤„ç†

åœ¨å°†è¿™äº›æ–‡æœ¬è¾“å…¥åˆ°æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œé¢„å¤„ç†ã€‚è¿™æ˜¯ç”±ğŸ¤—transformerçš„`Tokenizer`å®Œæˆçš„ï¼Œæ­£å¦‚å®ƒçš„åå­—æ‰€æš—ç¤ºçš„é‚£æ ·ï¼Œå®ƒå°†è¾“å…¥è¡¨ç¤ºä¸ºä¸€ç³»åˆ—tokenï¼Œç„¶åé€šè¿‡æŸ¥æ‰¾é¢„è®­ç»ƒå¥½çš„è¯æ±‡è¡¨ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºç›¸åº”çš„idã€‚æœ€åè½¬æ¢æˆæ¨¡å‹æ‰€æœŸæœ›çš„æ ¼å¼ï¼ŒåŒæ—¶ç”Ÿæˆæ¨¡å‹æ‰€éœ€çš„å…¶ä»–è¾“å…¥ã€‚

ä¸ºäº†åšåˆ°è¿™ä¸€åˆ‡ï¼Œæˆ‘ä»¬ä½¿ç”¨`AutoTokenizer`çš„`from_pretrained`æ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œå®ƒå°†ç¡®ä¿:

-æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå¯¹åº”äºæˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹æ¶æ„çš„tokenizerï¼Œ
-æˆ‘ä»¬ä¸‹è½½å¥½äº†é¢„è®­ç»ƒè¿™ä¸ªç‰¹å®šæ¨¡å‹æ—¶ä½¿ç”¨çš„è¯è¡¨ã€‚

åŒæ—¶ï¼Œè¯¥è¯è¡¨å°†è¢«ç¼“å­˜ï¼Œå› æ­¤ä¸‹æ¬¡è¿è¡Œæ—¶ä¸ä¼šå†æ¬¡ä¸‹è½½å®ƒã€‚


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```

æˆ‘ä»¬å°†`use_fast=True`ä½œä¸ºå‚æ•°å…¥ï¼Œä»¥ä½¿ç”¨ğŸ¤—tokenizersåº“ä¸­çš„ä¸€ä¸ªå¿«é€Ÿtokenizer(å®ƒç”±Rustæ”¯æŒçš„)ã€‚è¿™äº›å¿«é€Ÿtokenizerå‡ ä¹é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼Œä½†å¦‚æœæ‚¨åœ¨å‰é¢çš„è°ƒç”¨ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·åˆ é™¤è¯¥å‚æ•°ã€‚

ä½ å¯ä»¥ç›´æ¥åœ¨ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªå¥å­å¯¹ä¸Šè°ƒç”¨è¿™ä¸ªtokenizer:


```python
tokenizer("Hello, this one sentence!", "And this sentence goes with it.")
```




    {'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}



æ ¹æ®æ‚¨é€‰æ‹©çš„æ¨¡å‹ï¼Œæ‚¨å°†åœ¨ä¸Šé¢å•å…ƒæ ¼è¿”å›çš„å­—å…¸ä¸­çœ‹åˆ°ä¸åŒçš„é”®å€¼å¯¹ã€‚å®ƒä»¬å¯¹äºæˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„å¹¶ä¸é‡è¦ï¼Œåªéœ€è¦çŸ¥é“å®ƒä»¬æ˜¯æˆ‘ä»¬ç¨åå®ä¾‹åŒ–çš„æ¨¡å‹æ‰€éœ€è¦çš„ã€‚å¦‚æœæ‚¨å¯¹æ­¤æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨[æœ¬æ•™ç¨‹](https://huggingface.co/transformers/preprocessing.html)ä¸­äº†è§£æ›´å¤šå…³äºå®ƒä»¬çš„ä¿¡æ¯ã€‚

å¦‚ä¸‹é¢çš„å­—å…¸æ‰€ç¤ºï¼Œä¸ºäº†å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åŒ…å«å¥å­çš„åˆ—çš„åç§°:

æˆ‘ä»¬å¯ä»¥å†™ä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æˆ‘ä»¬çš„æ ·æœ¬ã€‚åœ¨è°ƒç”¨tokenizerä¹‹å‰ï¼Œæœ€æ£˜æ‰‹çš„éƒ¨åˆ†æ˜¯å°†æ‰€æœ‰å¯èƒ½çš„å¥å­å¯¹æ”¾åœ¨ä¸¤ä¸ªå¤§åˆ—è¡¨ä¸­ï¼Œç„¶åå°†ç»“æœæ‹‰å¹³ï¼Œä»¥ä¾¿æ¯ä¸ªç¤ºä¾‹æœ‰å››ä¸ªè¾“å…¥idã€æ³¨æ„åŠ›æ©ç ç­‰ã€‚

å½“è°ƒç”¨`tokenizer`æ—¶ï¼Œæˆ‘ä»¬ä¼ å…¥å‚æ•°`truncation=True`ã€‚è¿™å°†ç¡®ä¿æ¯”æ‰€é€‰æ¨¡å‹æ‰€èƒ½å¤„ç†çš„æ›´é•¿çš„è¾“å…¥å°†è¢«æˆªæ–­ä¸ºæ¨¡å‹æ‰€èƒ½æ¥å—çš„æœ€å¤§é•¿åº¦ã€‚


```python
ending_names = ["ending0", "ending1", "ending2", "ending3"]

def preprocess_function(examples):
    # Repeat each first sentence four times to go with the four possibilities of second sentences.
    first_sentences = [[context] * 4 for context in examples["sent1"]]
    # Grab all second sentences possible for each context.
    question_headers = examples["sent2"]
    second_sentences = [[f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)]
    
    # Flatten everything
    first_sentences = sum(first_sentences, [])
    second_sentences = sum(second_sentences, [])
    
    # Tokenize
    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
    # Un-flatten
    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```

This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists of lists for each key: a list of all examples (here 5), then a list of all choices (4) and a list of input IDs (length varying here since we did not apply any padding):

è¿™ä¸ªå‡½æ•°å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªç¤ºä¾‹ã€‚åœ¨ä¼ å…¥å¤šä¸ªç¤ºä¾‹æ—¶ï¼Œtokenizerå°†ä¸ºæ¯ä¸ªé”®è¿”å›ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼šæ‰€æœ‰ç¤ºä¾‹çš„åˆ—è¡¨(é•¿åº¦ä¸º5)ï¼Œç„¶åæ˜¯æ‰€æœ‰é€‰é¡¹çš„åˆ—è¡¨(é•¿åº¦ä¸º4)ä»¥åŠè¾“å…¥idçš„åˆ—è¡¨(é•¿åº¦ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰åº”ç”¨ä»»ä½•å¡«å……):


```python
examples = datasets["train"][:5]
features = preprocess_function(examples)
print(len(features["input_ids"]), len(features["input_ids"][0]), [len(x) for x in features["input_ids"][0]])
```

    5 4 [30, 25, 30, 28]


è®©æˆ‘ä»¬è§£ç ä¸€ä¸‹ç»™å®šç¤ºä¾‹çš„è¾“å…¥:


```python
idx = 3
[tokenizer.decode(features["input_ids"][idx][i]) for i in range(4)]
```




    ['[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP]',
     '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP]',
     '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions. [SEP]',
     '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession play and go back and forth hitting the drums while the audience claps for them. [SEP]']



æˆ‘ä»¬å¯ä»¥å°†å®ƒå’Œä¹‹å‰ç”Ÿæˆçš„ground truthè¿›è¡Œæ¯”è¾ƒï¼š


```python
show_one(datasets["train"][3])
```

    Context: A drum line passes by walking down the street playing their instruments.
      A - Members of the procession are playing ping pong and celebrating one left each in quick.
      B - Members of the procession wait slowly towards the cadets.
      C - Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.
      D - Members of the procession play and go back and forth hitting the drums while the audience claps for them.
    
    Ground truth: option D


è¿™ä¼¼ä¹æ²¡é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªå‡½æ•°åº”ç”¨åˆ°æˆ‘ä»¬æ•°æ®é›†çš„æ‰€æœ‰ç¤ºä¾‹ä¸­ï¼Œåªéœ€è¦ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰åˆ›å»ºçš„`dataset`å¯¹è±¡çš„`map`æ–¹æ³•ã€‚è¿™å°†åº”ç”¨äº`dataset`å¯¹è±¡çš„æ‰€æœ‰åˆ‡åˆ†çš„æ‰€æœ‰å…ƒç´ ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è®­ç»ƒï¼ŒéªŒè¯å’Œæµ‹è¯•æ•°æ®å°†ä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œé¢„å¤„ç†ã€‚


```python
encoded_datasets = datasets.map(preprocess_function, batched=True)
```

    Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-975c81cf12e5b7ac.arrow
    Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-d4806d63f1eaf5cd.arrow
    Loading cached processed dataset at /home/sgugger/.cache/huggingface/datasets/swag/regular/0.0.0/f9784740e0964a3c799d68cec0d992cc267d3fe94f3e048175eca69d739b980d/cache-258c9cd71b0182db.arrow


æ›´å¥½çš„æ˜¯ï¼Œç»“æœä¼šè¢«ğŸ¤—Datasetsåº“è‡ªåŠ¨ç¼“å­˜ï¼Œä»¥é¿å…ä¸‹æ¬¡è¿è¡Œæ—¶åœ¨è¿™ä¸€æ­¥ä¸ŠèŠ±è´¹æ—¶é—´ã€‚ğŸ¤—Datasetsåº“é€šå¸¸è¶³å¤Ÿæ™ºèƒ½ï¼Œå®ƒå¯ä»¥æ£€æµ‹ä¼ é€’ç»™`map`çš„å‡½æ•°ä½•æ—¶å‘ç”Ÿæ›´æ”¹(æ­¤æ—¶ä¸å†ä½¿ç”¨ç¼“å­˜æ•°æ®)ã€‚ä¾‹å¦‚ï¼Œå®ƒå°†æ£€æµ‹æ‚¨æ˜¯å¦åœ¨ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ä¸­æ›´æ”¹äº†ä»»åŠ¡å¹¶é‡æ–°è¿è¡Œç¬”è®°æœ¬ã€‚å½“ğŸ¤—Datasetsä½¿ç”¨ç¼“å­˜æ–‡ä»¶æ—¶ï¼Œå®ƒæç¤ºç›¸åº”çš„è­¦å‘Šï¼Œä½ å¯ä»¥åœ¨è°ƒç”¨`map`ä¸­ä¼ å…¥`load_from_cache_file=False`ä»è€Œä¸ä½¿ç”¨ç¼“å­˜æ–‡ä»¶ï¼Œå¹¶å¼ºåˆ¶è¿›è¡Œé¢„å¤„ç†ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¼ é€’äº†`batched=True`ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨æˆ‘ä»¬å‰é¢åŠ è½½çš„å¿«é€Ÿtokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚

## å¾®è°ƒæ¨¡å‹

ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚å› ä¸ºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯å…³äºå¤šé¡¹é€‰æ‹©çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨`AutoModelForMultipleChoice`ç±»ã€‚ä¸tokenizerä¸€æ ·ï¼Œ`from_pretrained`æ–¹æ³•å°†ä¸ºæˆ‘ä»¬ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹ã€‚


```python
from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)
```

    Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
    - This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


è¿™ä¸ªè­¦å‘Šå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡(`vocab_transform`å’Œ`vocab_layer_norm`å±‚)ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›å‚æ•°(`pre_classifier`å’Œ`classifier`å±‚)ã€‚è¿™æ˜¯å®Œå…¨æ­£å¸¸çš„æƒ…å†µï¼Œå› ä¸ºæˆ‘ä»¬èˆå¼ƒäº†åœ¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡çš„å¤´ï¼Œä»£ä¹‹ä»¥ä¸€ä¸ªæ–°çš„å¤šé€‰å¤´ï¼Œå¹¶ä¸”æˆ‘ä»¬æ²¡æœ‰å…¶é¢„è®­ç»ƒå¥½çš„æƒé‡ï¼Œæ‰€ä»¥è¿™ä¸ªè­¦å‘Šå‘Šè¯‰æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥æ¨ç†ä¹‹å‰éœ€è¦å¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„ã€‚

ä¸ºäº†å®ä¾‹åŒ–ä¸€ä¸ª`Trainer`ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å¦å¤–ä¸‰ä¸ªä¸œè¥¿ã€‚æœ€é‡è¦çš„æ˜¯[`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç”¨äºè®­ç»ƒçš„å±æ€§çš„ç±»ã€‚å®ƒéœ€è¦ä¼ å…¥ä¸€ä¸ªæ–‡ä»¶å¤¹åï¼Œç”¨äºä¿å­˜æ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œè€Œæ‰€æœ‰å…¶ä»–å‚æ•°éƒ½æ˜¯å¯é€‰çš„:


```python
args = TrainingArguments(
    "test-glue",
    evaluation_strategy = "epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾ç½®åœ¨æ¯ä¸ªepochçš„æœ«å°¾è¿›è¡Œè¯„ä¼°ï¼Œè°ƒæ•´å­¦ä¹ é€Ÿç‡ï¼Œä½¿ç”¨åœ¨jupyterç¬”è®°æœ¬é¡¶éƒ¨å®šä¹‰çš„`batch_size`ï¼Œå¹¶å®šåˆ¶ç”¨äºè®­ç»ƒçš„epochçš„æ•°é‡ï¼Œä»¥åŠæƒé‡è¡°å‡ã€‚

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰æˆ‘ä»¬çš„`Trainer`å¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ æ‰¹æ•°æ®ã€‚æˆ‘ä»¬è¿˜æ²¡æœ‰åšä»»ä½•å¡«å……ï¼Œå› ä¸ºæˆ‘ä»¬å°†å¡«å……æ¯ä¸ªæ‰¹åˆ°æ‰¹å†…çš„æœ€å¤§é•¿åº¦(è€Œä¸æ˜¯ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦)ã€‚è¿™å°†æ˜¯*data collator*çš„å·¥ä½œã€‚å®ƒæ¥å—ç¤ºä¾‹çš„åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºä¸€ä¸ªæ‰¹(åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œé€šè¿‡åº”ç”¨å¡«å……)ã€‚ç”±äºåœ¨åº“ä¸­æ²¡æœ‰data collatoræ¥å¤„ç†æˆ‘ä»¬çš„ç‰¹å®šé—®é¢˜ï¼Œè¿™é‡Œæˆ‘ä»¬æ ¹æ®`DataCollatorWithPadding`è‡ªè¡Œæ”¹ç¼–ä¸€ä¸ª:


```python
from dataclasses import dataclass
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
from typing import Optional, Union
import torch

@dataclass
class DataCollatorForMultipleChoice:
    """
    Data collator that will dynamically pad the inputs for multiple choice received.
    """

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0]["input_ids"])
        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]
        flattened_features = sum(flattened_features, [])
        
        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        
        # Un-flatten
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        batch["labels"] = torch.tensor(labels, dtype=torch.int64)
        return batch
```

å½“ä¼ å…¥ä¸€ä¸ªç¤ºä¾‹çš„åˆ—è¡¨æ—¶ï¼Œå®ƒä¼šå°†å¤§åˆ—è¡¨ä¸­çš„æ‰€æœ‰è¾“å…¥/æ³¨æ„åŠ›æ©ç ç­‰éƒ½å‹å¹³ï¼Œå¹¶ä¼ é€’ç»™`tokenizer.pad`æ–¹æ³•ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå¸¦æœ‰å¤§å¼ é‡çš„å­—å…¸(å…¶å¤§å°ä¸º`(batch_size * 4) x seq_length`)ï¼Œç„¶åæˆ‘ä»¬å°†å…¶å±•å¼€ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨ç‰¹å¾åˆ—è¡¨ä¸Šæ£€æŸ¥data collatoræ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªéœ€è¦ç¡®ä¿åˆ é™¤æ‰€æœ‰ä¸è¢«æˆ‘ä»¬çš„æ¨¡å‹æ¥å—çš„è¾“å…¥ç‰¹å¾(è¿™æ˜¯`Trainer`è‡ªåŠ¨ä¸ºæˆ‘ä»¬åšçš„)ï¼š


```python
accepted_keys = ["input_ids", "attention_mask", "label"]
features = [{k: v for k, v in encoded_datasets["train"][i].items() if k in accepted_keys} for i in range(10)]
batch = DataCollatorForMultipleChoice(tokenizer)(features)
```

å†æ¬¡å¼ºè°ƒï¼Œæ‰€æœ‰è¿™äº›å‹å¹³çš„ã€æœªå‹å¹³çš„éƒ½å¯èƒ½æ˜¯æ½œåœ¨é”™è¯¯çš„æ¥æºï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å¯¹è¾“å…¥è¿›è¡Œå¦ä¸€ä¸ªå®Œæ•´æ€§æ£€æŸ¥ï¼š


```python
[tokenizer.decode(batch["input_ids"][8][i].tolist()) for i in range(4)]
```




    ['[CLS] someone walks over to the radio. [SEP] someone hands her another phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
     '[CLS] someone walks over to the radio. [SEP] someone takes the drink, then holds it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
     '[CLS] someone walks over to the radio. [SEP] someone looks off then looks at someone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
     '[CLS] someone walks over to the radio. [SEP] someone stares blearily down at the floor. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']




```python
show_one(datasets["train"][8])
```

    Context: Someone walks over to the radio.
      A - Someone hands her another phone.
      B - Someone takes the drink, then holds it.
      C - Someone looks off then looks at someone.
      D - Someone stares blearily down at the floor.
    
    Ground truth: option D


æ‰€æœ‰çš„éƒ½æ­£å¸¸è¿è¡Œ!

æœ€åè¦ä¸º`Trainer`å®šä¹‰å¦‚ä½•æ ¹æ®é¢„æµ‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬éœ€è¦æ¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰åŠ è½½çš„`metric`ï¼Œæˆ‘ä»¬å¿…é¡»åšçš„å”¯ä¸€é¢„å¤„ç†æ˜¯å–æˆ‘ä»¬é¢„æµ‹çš„logitsçš„argmaxï¼š


```python
import numpy as np

def compute_metrics(eval_predictions):
    predictions, label_ids = eval_predictions
    preds = np.argmax(predictions, axis=1)
    return {"accuracy": (preds == label_ids).astype(np.float32).mean().item()}
```

ç„¶åï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ‰€æœ‰è¿™äº›ä»¥åŠæˆ‘ä»¬çš„æ•°æ®é›†ä¸€èµ·ä¼ å…¥`Trainer`ï¼š


```python
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_datasets["train"],
    eval_dataset=encoded_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer),
    compute_metrics=compute_metrics,
)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨`train`æ–¹æ³•æ¥å¾®è°ƒæ¨¡å‹ï¼š


```python
trainer.train()
```



    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
        </style>

      <progress value='6897' max='6897' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [6897/6897 23:49, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.154598</td>
      <td>0.828017</td>
      <td>0.766520</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.296633</td>
      <td>0.667454</td>
      <td>0.786814</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.111786</td>
      <td>0.994927</td>
      <td>0.789363</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=6897, training_loss=0.19714653808275168)



æœ€åï¼Œä¸è¦å¿˜è®°å°†ä½ çš„æ¨¡å‹[ä¸Šä¼ ](https://huggingface.co/transformers/model_sharing.html)åˆ°[ğŸ¤— æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚
