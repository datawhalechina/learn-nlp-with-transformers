æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨[ç¯‡ç« 4ä»£ç åº“ä¸­](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)ã€‚

å»ºè®®ç›´æ¥ä½¿ç”¨google colab notebookæ‰“å¼€æœ¬æ•™ç¨‹ï¼Œå¯ä»¥å¿«é€Ÿä¸‹è½½ç›¸å…³æ•°æ®é›†å’Œæ¨¡å‹ã€‚
å¦‚æœæ‚¨æ­£åœ¨googleçš„colabä¸­æ‰“å¼€è¿™ä¸ªnotebookï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…Transformerså’ŒğŸ¤—Datasetsåº“ã€‚å°†ä»¥ä¸‹å‘½ä»¤å–æ¶ˆæ³¨é‡Šå³å¯å®‰è£…ã€‚


```python
# ! pip install datasets transformers 
# -i https://pypi.tuna.tsinghua.edu.cn/simple
```

å¦‚æœæ‚¨æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰“å¼€è¿™ä¸ªjupyterç¬”è®°æœ¬ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå®‰è£…äº†ä¸Šè¿°åº“çš„æœ€æ–°ç‰ˆæœ¬ã€‚

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)æ‰¾åˆ°è¿™ä¸ªjupyterç¬”è®°æœ¬çš„å…·ä½“çš„pythonè„šæœ¬æ–‡ä»¶ï¼Œè¿˜å¯ä»¥é€šè¿‡åˆ†å¸ƒå¼çš„æ–¹å¼ä½¿ç”¨å¤šä¸ªgpuæˆ–tpuæ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

# å¾®è°ƒè¯­è¨€æ¨¡å‹

åœ¨å½“å‰jupyterç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹ä»»åŠ¡å¾®è°ƒä»»æ„[ğŸ¤—Transformers](https://github.com/huggingface/transformers) æ¨¡å‹ã€‚ 

æœ¬æ•™ç¨‹å°†æ¶µç›–ä¸¤ç§ç±»å‹çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡:

+ å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal language modelingï¼ŒCLMï¼‰ï¼šæ¨¡å‹éœ€è¦é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä½ç½®å¤„çš„å­—ç¬¦ï¼ˆç±»ä¼¼BERTç±»æ¨¡å‹çš„decoderå’ŒGPTï¼Œä»å·¦å¾€å³è¾“å…¥å­—ç¬¦ï¼‰ã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹ä¸ä½œå¼Šï¼Œæ¨¡å‹ä¼šä½¿ç”¨ä¸€ä¸ªæ³¨æ„æ©ç é˜²æ­¢æ¨¡å‹çœ‹åˆ°ä¹‹åçš„å­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¯•å›¾é¢„æµ‹å¥å­ä¸­çš„i+1ä½ç½®å¤„çš„å­—ç¬¦æ—¶ï¼Œè¿™ä¸ªæ©ç å°†é˜»æ­¢å®ƒè®¿é—®iä½ç½®ä¹‹åçš„å­—ç¬¦ã€‚

![æ¨ç†è¡¨ç¤ºå› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡å›¾ç‰‡](./images/causal_language_modeling.png)

+ æ©è”½è¯­è¨€å»ºæ¨¡ï¼ˆMasked language modelingï¼ŒMLMï¼‰ï¼šæ¨¡å‹éœ€è¦æ¢å¤è¾“å…¥ä¸­è¢«"MASK"æ‰çš„ä¸€äº›å­—ç¬¦ï¼ˆBERTç±»æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼‰ã€‚è¿™ç§æ–¹å¼æ¨¡å‹å¯ä»¥çœ‹åˆ°æ•´ä¸ªå¥å­ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥æ ¹æ®â€œ\[MASK\]â€æ ‡è®°ä¹‹å‰å’Œä¹‹åçš„å­—ç¬¦æ¥é¢„æµ‹è¯¥ä½ç½®è¢«â€œ\[MASK\]â€ä¹‹å‰çš„å­—ç¬¦ã€‚

![Widget inference representing the masked language modeling task](images/masked_language_modeling.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•è½»æ¾åœ°ä¸ºæ¯ä¸ªä»»åŠ¡åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨â€œTrainerâ€APIå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥åœ¨åˆ†å¸ƒå¼ç¯å¢ƒæˆ–TPUä¸Šè¿è¡Œè¯¥jupyterç¬”è®°æœ¬çš„pythonè„šæœ¬ç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨[examplesæ–‡ä»¶å¤¹](https://github.com/huggingface/transformers/tree/master/examples)ä¸­æ‰¾åˆ°ã€‚

## å‡†å¤‡æ•°æ®

åœ¨æ¥ä¸‹æ¥çš„è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Wikitext 2](https://huggingface.co/datasets/wikitext#data-instances)æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ã€‚æ‚¨å¯ä»¥é€šè¿‡ğŸ¤—Datasetsåº“åŠ è½½è¯¥æ•°æ®é›†ï¼š


```python
from datasets import load_dataset
datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
```

    Reusing dataset wikitext (/Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)


å¦‚æœç¢°åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
![request Error](images/request_error.png)

è§£å†³æ–¹æ¡ˆ:

MACç”¨æˆ·: åœ¨ ```/etc/hosts``` æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```

Windowsoç”¨æˆ·: åœ¨ ```C:\Windows\System32\drivers\etc\hosts```  æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```

å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ç”¨å…¬å¼€åœ¨[hub](https://huggingface.co/datasets)ä¸Šçš„ä»»ä½•æ•°æ®é›†æ›¿æ¢ä¸Šé¢çš„æ•°æ®é›†ï¼Œæˆ–è€…ä½¿ç”¨æ‚¨è‡ªå·±çš„æ–‡ä»¶ã€‚åªéœ€å–æ¶ˆæ³¨é‡Šä»¥ä¸‹å•å…ƒæ ¼ï¼Œå¹¶å°†è·¯å¾„æ›¿æ¢ä¸ºå°†å¯¼è‡´æ‚¨çš„æ–‡ä»¶è·¯å¾„ï¼š


```python
# datasets = load_dataset("text", data_files={"train": path_to_train.txt, "validation": path_to_validation.txt}
```

æ‚¨è¿˜å¯ä»¥ä»csvæˆ–JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚é˜…[å®Œæ•´æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)ã€‚

è¦è®¿é—®ä¸€ä¸ªæ•°æ®ä¸­å®é™…çš„å…ƒç´ ï¼Œæ‚¨éœ€è¦å…ˆé€‰æ‹©ä¸€ä¸ªkeyï¼Œç„¶åç»™å‡ºä¸€ä¸ªç´¢å¼•:


```python
datasets["train"][10]
```




    {'text': ' The game \'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific " Potentials " , skills unique to each character . They are divided into " Personal Potential " , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and " Battle Potentials " , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique " Masters Table " , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate " Direct Command " and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her " Valkyria Form " and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n'}



ä¸ºäº†å¿«é€Ÿäº†è§£æ•°æ®çš„ç»“æ„ï¼Œä¸‹é¢çš„å‡½æ•°å°†æ˜¾ç¤ºæ•°æ®é›†ä¸­éšæœºé€‰å–çš„ä¸€äº›ç¤ºä¾‹ã€‚


```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>On 3 March 1967 , parliament decided to build four short take @-@ off and landing airports along the Helgeland coast between Trondheim and BodÃ¸ . Braathens placed an order for a de Havilland Canada DHC @-@ 6 Twin Otter and planned to start the company Braathens STOL . It applied to operate the route without subsidies , but the concession was rejected and granted with subsidies to WiderÃ¸e , which had been operating the routes using seaplanes . \n</td>
    </tr>
    <tr>
      <th>1</th>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Rao Ramesh was cast as a tantrik who helps Gill 's character in the present era . Mumaith Khan was selected for another item number , a remix version of the hit song " Bangaru Kodipetta " from Gharana Mogudu ( 1992 ) ; Gharana Mogudu 's music was also composed by M. M. Keeravani . Chiranjeevi made a special appearance after the song , making Magadheera the first film he appeared in after his entry into politics . When Rajamouli suggested the idea of a cameo appearance , Chiranjeevi was initially hesitant till the director narrated the complete sequence and the importance of the song . \n</td>
    </tr>
    <tr>
      <th>3</th>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>= = = Total Nonstop Action Wrestling ( 2015 â€“ present ) = = = \n</td>
    </tr>
    <tr>
      <th>6</th>
      <td>The Daily Telegraph gave the visual novel the award for " Best Script " in its video game awards of 2011 , stating that " Love 's layered narrative of a high school teacher embroiled in his student â€™ s worries goes places most mainstream video games wouldn 't dare . " \n</td>
    </tr>
    <tr>
      <th>7</th>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td></td>
    </tr>
  </tbody>
</table>


æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä¸€äº›æ–‡æœ¬æ˜¯ç»´åŸºç™¾ç§‘æ–‡ç« çš„å®Œæ•´æ®µè½ï¼Œè€Œå…¶ä»–çš„åªæ˜¯æ ‡é¢˜æˆ–ç©ºè¡Œã€‚

## å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelingï¼ŒCLMï¼‰

å¯¹äºå› æœè¯­è¨€æ¨¡å‹(CLM)ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬ï¼Œå¹¶åœ¨å®ƒä»¬è¢«åˆ†è¯åå°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å°†æ¥æ”¶å¦‚ä¸‹çš„è¿ç»­æ–‡æœ¬å—:

```
æ–‡æœ¬1
```
æˆ–
```
æ–‡æœ¬1ç»“å°¾ [BOS_TOKEN] æ–‡æœ¬2å¼€å¤´
```

å–å†³äºå®ƒä»¬æ˜¯å¦è·¨è¶Šæ•°æ®é›†ä¸­çš„å‡ ä¸ªåŸå§‹æ–‡æœ¬ã€‚æ ‡ç­¾å°†ä¸è¾“å…¥ç›¸åŒï¼Œä½†å‘å·¦ç§»åŠ¨ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[`distilgpt2`](https://huggingface.co/distilgpt2) æ¨¡å‹ã€‚æ‚¨åŒæ ·ä¹Ÿå¯ä»¥é€‰æ‹©[è¿™é‡Œ](https://huggingface.co/models?filter=causal-lm)åˆ—å‡ºçš„ä»»ä½•ä¸€ä¸ªcheckpoint:


```python
model_checkpoint = "distilgpt2"
```

ä¸ºäº†ç”¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„è¯æ±‡å¯¹æ‰€æœ‰æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œæˆ‘ä»¬å¿…é¡»ä¸‹è½½ä¸€ä¸ªé¢„å…ˆè®­ç»ƒè¿‡çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ã€‚è€Œè¿™äº›æ“ä½œéƒ½å¯ä»¥ç”±AutoTokenizerç±»å®Œæˆ:


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ‰€æœ‰çš„æ–‡æœ¬è°ƒç”¨åˆ†è¯å™¨ï¼Œè¯¥æ“ä½œå¯ä»¥ç®€å•åœ°ä½¿ç”¨æ¥è‡ªDatasetsåº“çš„mapæ–¹æ³•å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåœ¨æ–‡æœ¬ä¸Šè°ƒç”¨æ ‡è®°å™¨çš„å‡½æ•°:


```python
def tokenize_function(examples):
    return tokenizer(examples["text"])
```

ç„¶åæˆ‘ä»¬å°†å®ƒåº”ç”¨åˆ°datasetså¯¹è±¡ä¸­çš„åˆ†è¯ï¼Œä½¿ç”¨```batch=True```å’Œ```4```ä¸ªè¿›ç¨‹æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚è€Œä¹‹åæˆ‘ä»¬å¹¶ä¸éœ€è¦```text```åˆ—ï¼Œæ‰€ä»¥å°†å…¶èˆå¼ƒã€‚



```python
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
```

å¦‚æœæˆ‘ä»¬ç°åœ¨æŸ¥çœ‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æ–‡æœ¬å·²ç»è¢«æ¨¡å‹æ‰€éœ€çš„input_idsæ‰€å–ä»£:


```python
tokenized_datasets["train"][1]
```




    {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],
     'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]}



ä¸‹ä¸€æ­¥å°±æœ‰ç‚¹å°å›°éš¾äº†ï¼šæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åå°†ç»“æœåˆ†å‰²æˆç‰¹å®š`block_size`çš„å°å—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨`map`æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨é€‰é¡¹`batch=True`ã€‚è¿™ä¸ªé€‰é¡¹å…è®¸æˆ‘ä»¬é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è·å–é¢„è®­ç»ƒæ¨¡å‹æ—¶æ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚æœ€å¤§é•¿åº¦åœ¨è¿™é‡Œè®¾ç½®ä¸º128ï¼Œä»¥é˜²æ‚¨çš„æ˜¾å­˜çˆ†ç‚¸ğŸ’¥ã€‚


```python
# block_size = tokenizer.model_max_length
block_size = 128
```

ç„¶åæˆ‘ä»¬ç¼–å†™é¢„å¤„ç†å‡½æ•°æ¥å¯¹æˆ‘ä»¬çš„æ–‡æœ¬è¿›è¡Œåˆ†ç»„:


```python
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚
    total_length = (total_length // block_size) * block_size
    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```

é¦–å…ˆæ³¨æ„ï¼Œæˆ‘ä»¬å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ã€‚

è¿™æ˜¯å› ä¸ºğŸ¤—transformeråº“çš„æ¨¡å‹é»˜è®¤å‘å³ç§»åŠ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨æ“ä½œã€‚

è¿˜è¦æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œ`map`æ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ é™¤å‰©ä½™éƒ¨åˆ†ï¼Œä½¿è¿æ¥çš„æ ‡è®°åŒ–æ–‡æœ¬æ¯1000ä¸ªç¤ºä¾‹ä¸º`block_size`çš„å€æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡ä¼ é€’æ›´é«˜çš„æ‰¹å¤„ç†å¤§å°æ¥è°ƒæ•´æ­¤è¡Œä¸º(å½“ç„¶è¿™ä¹Ÿä¼šè¢«å¤„ç†å¾—æ›´æ…¢)ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`multiprocessing`æ¥åŠ é€Ÿé¢„å¤„ç†:


```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ï¼šç°åœ¨æ ·æœ¬åŒ…å«äº†`block_size`è¿ç»­å­—ç¬¦å—ï¼Œå¯èƒ½è·¨è¶Šäº†å‡ ä¸ªåŸå§‹æ–‡æœ¬ã€‚


```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```




    ' game and follows the " Nameless ", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " Calamaty Raven ". \n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'



æ—¢ç„¶æ•°æ®å·²ç»æ¸…ç†å®Œæ¯•ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®ä¾‹åŒ–æˆ‘ä»¬çš„è®­ç»ƒå™¨äº†ã€‚æˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªæ¨¡å‹:


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
```

æ£€æŸ¥torchç‰ˆæœ¬


```python

import importlib.util
# import importlib_metadata
a = importlib.util.find_spec("torch") is not None
print(a)
# _torch_version = importlib_metadata.version("torch")
# print(_torch_version)
```

    True


å’Œä¸€äº›`TrainingArguments`:


```python
from transformers import Trainer, TrainingArguments
```


```python
training_args = TrainingArguments(
    "test-clm",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)
```

æˆ‘ä»¬æŠŠè¿™äº›éƒ½ä¼ é€’ç»™`Trainer`ç±»:


```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"],
    eval_dataset=lm_datasets["validation"],
)
```

ç„¶åå°±å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ğŸŒ¶:


```python
trainer.train()
```

      0%|          | 31/7002 [04:16<14:27:52,  7.47s/it]

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¾—åˆ°å®ƒåœ¨éªŒè¯é›†ä¸Šçš„perplexityï¼Œå¦‚ä¸‹æ‰€ç¤º:


```python
import math
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

## æ©è”½è¯­è¨€æ¨¡å‹ï¼ˆMask Language Modelingï¼ŒMLMï¼‰

æ©è”½è¯­è¨€æ¨¡å‹(MLM)æˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†é¢„å¤„ç†å’Œä»¥å‰ä¸€æ ·ç”¨ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ï¼š

æˆ‘ä»¬å°†éšæœº"MASK"ä¸€äº›å­—ç¬¦(ä½¿ç”¨"[MASK]"è¿›è¡Œæ›¿æ¢)ä»¥åŠè°ƒæ•´æ ‡ç­¾ä¸ºåªåŒ…å«åœ¨"[MASK]"ä½ç½®å¤„çš„æ ‡ç­¾(å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦é¢„æµ‹æ²¡æœ‰è¢«"MASK"çš„å­—ç¬¦)ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[`distilroberta-base`](https://huggingface.co/distilroberta-base)æ¨¡å‹ã€‚æ‚¨åŒæ ·ä¹Ÿå¯ä»¥é€‰æ‹©[è¿™é‡Œ](https://huggingface.co/models?filter=causal-lm)åˆ—å‡ºçš„ä»»ä½•ä¸€ä¸ªcheckpoint:


```python
model_checkpoint = "distilroberta-base"
```

æˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·åº”ç”¨ç›¸åŒçš„åˆ†è¯å™¨å‡½æ•°ï¼Œæˆ‘ä»¬åªéœ€è¦æ›´æ–°æˆ‘ä»¬çš„åˆ†è¯å™¨æ¥ä½¿ç”¨åˆšåˆšé€‰æ‹©çš„checkpoint:


```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
```

åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬æŠŠæ–‡æœ¬åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå¹¶æŠŠå®ƒä»¬åˆ†æˆé•¿åº¦ä¸º`block_size`çš„æ ·æœ¬ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±å•ç‹¬çš„å¥å­ç»„æˆï¼Œåˆ™å¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ã€‚


```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```

å‰©ä¸‹çš„å’Œæˆ‘ä»¬ä¹‹å‰çš„åšæ³•éå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ä¸¤ä¸ªä¾‹å¤–ã€‚é¦–å…ˆæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€‚åˆæ©è”½è¯­è¨€æ¨¡å‹çš„æ¨¡å‹:


```python
from transformers import AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„data_collatorã€‚data_collatoræ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šçš„äº‹æƒ…è¦åšï¼Œæ‰€ä»¥æˆ‘ä»¬åªä½¿ç”¨è¿™ä¸ªå‚æ•°çš„é»˜è®¤å€¼ã€‚è¿™é‡Œæˆ‘ä»¬è¦åšéšæœº"MASK"ã€‚

æˆ‘ä»¬å¯ä»¥å°†å…¶ä½œä¸ºé¢„å¤„ç†æ­¥éª¤(`tokenizer`)è¿›è¡Œå¤„ç†ï¼Œä½†åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå­—ç¬¦æ€»æ˜¯ä»¥ç›¸åŒçš„æ–¹å¼è¢«æ©ç›–ã€‚é€šè¿‡åœ¨data_collatorä¸­æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ¯æ¬¡æ£€æŸ¥æ•°æ®æ—¶éƒ½ä»¥æ–°çš„æ–¹å¼å®Œæˆéšæœºæ©è”½ã€‚

ä¸ºäº†å®ç°æ©è”½ï¼Œ`Transformers`ä¸ºæ©è”½è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ª`DataCollatorForLanguageModeling`ã€‚æˆ‘ä»¬å¯ä»¥è°ƒæ•´æ©è”½çš„æ¦‚ç‡:


```python
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ç„¶åæˆ‘ä»¬è¦æŠŠæ‰€æœ‰çš„ä¸œè¥¿äº¤ç»™trainerï¼Œç„¶åå¼€å§‹è®­ç»ƒ:


```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"],
    eval_dataset=lm_datasets["validation"],
    data_collator=data_collator,
)
```


```python
trainer.train()
```

åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

ä¸CLMç›®æ ‡ç›¸æ¯”ï¼Œå›°æƒ‘åº¦è¦ä½å¾—å¤šï¼Œå› ä¸ºå¯¹äºMLMç›®æ ‡ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹éšè—çš„ä»¤ç‰Œ(åœ¨è¿™é‡Œå æ€»æ•°çš„15%)è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶å¯ä»¥è®¿é—®å…¶ä½™çš„ä»¤ç‰Œã€‚

å› æ­¤ï¼Œå¯¹äºæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€é¡¹æ›´å®¹æ˜“çš„ä»»åŠ¡ã€‚


```python
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```


```python
ä¸è¦å¿˜è®°å°†ä½ çš„æ¨¡å‹[ä¸Šä¼ ](https://huggingface.co/transformers/model_sharing.html)åˆ°[ğŸ¤— æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚
```
