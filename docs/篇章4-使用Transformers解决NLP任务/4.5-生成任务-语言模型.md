æœ¬æ–‡æ¶‰åŠçš„jupter notebookåœ¨[ç¯‡ç« 4ä»£ç åº“ä¸­](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)ã€‚

å»ºè®®ç›´æ¥ä½¿ç”¨google colab notebookæ‰“å¼€æœ¬æ•™ç¨‹ï¼Œå¯ä»¥å¿«é€Ÿä¸‹è½½ç›¸å…³æ•°æ®é›†å’Œæ¨¡å‹ã€‚
å¦‚æœæ‚¨æ­£åœ¨googleçš„colabä¸­æ‰“å¼€è¿™ä¸ªnotebookï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…Transformerså’ŒğŸ¤—Datasetsåº“ã€‚å°†ä»¥ä¸‹å‘½ä»¤å–æ¶ˆæ³¨é‡Šå³å¯å®‰è£…ã€‚


```python
# ! pip install datasets transformers 
# -i https://pypi.tuna.tsinghua.edu.cn/simple
```

å¦‚æœæ‚¨æ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰“å¼€è¿™ä¸ªjupyterç¬”è®°æœ¬ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå®‰è£…äº†ä¸Šè¿°åº“çš„æœ€æ–°ç‰ˆæœ¬ã€‚

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)æ‰¾åˆ°è¿™ä¸ªjupyterç¬”è®°æœ¬çš„å…·ä½“çš„pythonè„šæœ¬æ–‡ä»¶ï¼Œè¿˜å¯ä»¥é€šè¿‡åˆ†å¸ƒå¼çš„æ–¹å¼ä½¿ç”¨å¤šä¸ªgpuæˆ–tpuæ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

# å¾®è°ƒè¯­è¨€æ¨¡å‹

åœ¨å½“å‰jupyterç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹ä»»åŠ¡å¾®è°ƒä»»æ„[ğŸ¤—Transformers](https://github.com/huggingface/transformers) æ¨¡å‹ã€‚ 

æœ¬æ•™ç¨‹å°†æ¶µç›–ä¸¤ç§ç±»å‹çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡:

+ å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal language modelingï¼ŒCLMï¼‰ï¼šæ¨¡å‹éœ€è¦é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä½ç½®å¤„çš„å­—ç¬¦ï¼ˆç±»ä¼¼BERTç±»æ¨¡å‹çš„decoderå’ŒGPTï¼Œä»å·¦å¾€å³è¾“å…¥å­—ç¬¦ï¼‰ã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹ä¸ä½œå¼Šï¼Œæ¨¡å‹ä¼šä½¿ç”¨ä¸€ä¸ªæ³¨æ„æ©ç é˜²æ­¢æ¨¡å‹çœ‹åˆ°ä¹‹åçš„å­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¯•å›¾é¢„æµ‹å¥å­ä¸­çš„i+1ä½ç½®å¤„çš„å­—ç¬¦æ—¶ï¼Œè¿™ä¸ªæ©ç å°†é˜»æ­¢å®ƒè®¿é—®iä½ç½®ä¹‹åçš„å­—ç¬¦ã€‚

![æ¨ç†è¡¨ç¤ºå› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡å›¾ç‰‡](./images/causal_language_modeling.png)

+ æ©è”½è¯­è¨€å»ºæ¨¡ï¼ˆMasked language modelingï¼ŒMLMï¼‰ï¼šæ¨¡å‹éœ€è¦æ¢å¤è¾“å…¥ä¸­è¢«"MASK"æ‰çš„ä¸€äº›å­—ç¬¦ï¼ˆBERTç±»æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼‰ã€‚è¿™ç§æ–¹å¼æ¨¡å‹å¯ä»¥çœ‹åˆ°æ•´ä¸ªå¥å­ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥æ ¹æ®â€œ\[MASK\]â€æ ‡è®°ä¹‹å‰å’Œä¹‹åçš„å­—ç¬¦æ¥é¢„æµ‹è¯¥ä½ç½®è¢«â€œ\[MASK\]â€ä¹‹å‰çš„å­—ç¬¦ã€‚

![Widget inference representing the masked language modeling task](images/masked_language_modeling.png)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•è½»æ¾åœ°ä¸ºæ¯ä¸ªä»»åŠ¡åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨â€œTrainerâ€APIå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥åœ¨åˆ†å¸ƒå¼ç¯å¢ƒæˆ–TPUä¸Šè¿è¡Œè¯¥jupyterç¬”è®°æœ¬çš„pythonè„šæœ¬ç‰ˆæœ¬ï¼Œå¯ä»¥åœ¨[examplesæ–‡ä»¶å¤¹](https://github.com/huggingface/transformers/tree/master/examples)ä¸­æ‰¾åˆ°ã€‚

## å‡†å¤‡æ•°æ®

åœ¨æ¥ä¸‹æ¥çš„è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Wikitext 2](https://huggingface.co/datasets/wikitext#data-instances)æ•°æ®é›†ä½œä¸ºç¤ºä¾‹ã€‚æ‚¨å¯ä»¥é€šè¿‡ğŸ¤—Datasetsåº“åŠ è½½è¯¥æ•°æ®é›†ï¼š


```python
from datasets import load_dataset
datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
```

    Downloading: 8.33kB [00:00, 1.49MB/s]                   
    Downloading: 5.83kB [00:00, 1.77MB/s]                   


    Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.91 MiB, post-processed: Unknown size, total: 17.41 MiB) to /Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20...


    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.72M/4.72M [00:02<00:00, 1.91MB/s]


    Dataset wikitext downloaded and prepared to /Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20. Subsequent calls will reuse this data.


å¦‚æœç¢°åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
![request Error](images/request_error.png)

è§£å†³æ–¹æ¡ˆ:

MACç”¨æˆ·: åœ¨ ```/etc/hosts``` æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```

Windowsoç”¨æˆ·: åœ¨ ```C:\Windows\System32\drivers\etc\hosts```  æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```

å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥ç”¨å…¬å¼€åœ¨[hub](https://huggingface.co/datasets)ä¸Šçš„ä»»ä½•æ•°æ®é›†æ›¿æ¢ä¸Šé¢çš„æ•°æ®é›†ï¼Œæˆ–è€…ä½¿ç”¨æ‚¨è‡ªå·±çš„æ–‡ä»¶ã€‚åªéœ€å–æ¶ˆæ³¨é‡Šä»¥ä¸‹å•å…ƒæ ¼ï¼Œå¹¶å°†è·¯å¾„æ›¿æ¢ä¸ºå°†å¯¼è‡´æ‚¨çš„æ–‡ä»¶è·¯å¾„ï¼š


```python
# datasets = load_dataset("text", data_files={"train": path_to_train.txt, "validation": path_to_validation.txt}
```

æ‚¨è¿˜å¯ä»¥ä»csvæˆ–JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚é˜…[å®Œæ•´æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)ã€‚

è¦è®¿é—®ä¸€ä¸ªæ•°æ®ä¸­å®é™…çš„å…ƒç´ ï¼Œæ‚¨éœ€è¦å…ˆé€‰æ‹©ä¸€ä¸ªkeyï¼Œç„¶åç»™å‡ºä¸€ä¸ªç´¢å¼•:


```python
datasets["train"][10]
```




    {'text': ' The game \'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific " Potentials " , skills unique to each character . They are divided into " Personal Potential " , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and " Battle Potentials " , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique " Masters Table " , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate " Direct Command " and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her " Valkyria Form " and become invincible , while Imca can target multiple enemy units with her heavy weapon . \n'}



ä¸ºäº†å¿«é€Ÿäº†è§£æ•°æ®çš„ç»“æ„ï¼Œä¸‹é¢çš„å‡½æ•°å°†æ˜¾ç¤ºæ•°æ®é›†ä¸­éšæœºé€‰å–çš„ä¸€äº›ç¤ºä¾‹ã€‚


```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Plum cakes made with fresh plums came with other migrants from other traditions in which plum cake is prepared using plum as a primary ingredient . In some versions , the plums may become jam @-@ like inside the cake after cooking , or be prepared using plum jam . Plum cake prepared with plums is also a part of Ashkenazi Jewish cuisine , and is referred to as Pflaumenkuchen or Zwetschgenkuchen . Other plum @-@ based cakes are found in French , Italian and Polish cooking . \n</td>
    </tr>
    <tr>
      <th>1</th>
      <td>= = = Language = = = \n</td>
    </tr>
    <tr>
      <th>2</th>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>The town 's population not only recovered but grew ; the 1906 census of the Canadian Prairies listed the population at 1 @,@ 178 . A new study commissioned by the Dominion government determined that the cracks in the mountain continued to grow and that the risk of another slide remained . Consequently , parts of Frank closest to the mountain were dismantled or relocated to safer areas . \n</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The Litigators is a 2011 legal thriller novel by John Grisham , his 25th fiction novel overall . The Litigators is about a two @-@ partner Chicago law firm attempting to strike it rich in a class action lawsuit over a cholesterol reduction drug by a major pharmaceutical drug company . The protagonist is a Harvard Law School grad big law firm burnout who stumbles upon the boutique and joins it only to find himself litigating against his old law firm in this case . The book is regarded as more humorous than most of Grisham 's prior novels . \n</td>
    </tr>
    <tr>
      <th>6</th>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>On December 7 , 2006 , Headquarters Marine Corps released a message stating that 2nd Battalion 9th Marines would be reactivated during 2007 as part of the continuing Global War on Terror . 2nd Battalion 9th Marines was re @-@ activated on July 13 , 2007 and replaced the Anti @-@ Terrorism Battalion ( ATBn ) . In September 2008 , Marines and Sailors from 2 / 9 deployed to Al Anbar Province in support of Operation Iraqi Freedom . They were based in the city of Ramadi and returned in April 2009 without any Marines or Sailors killed in action . July 2010 Marines and Sailors from 2 / 9 deployed to Marjah , Helmand Province , Afghanistan in support of Operation Enduring Freedom . In December 2010 Echo Company from 2 / 9 were attached to 3 / 5 in Sangin , Afghanistan where they earned the notorious nickname of " Green Hats . " They returned February 2011 . They redeployed back to Marjah December 2011 and returned July 2012 . Echo and Weapons companies deployed once more to Afghanistan from January through April 2013 , participating in combat operations out of Camp Leatherneck . On April 1 , 2015 the battalion was deactivated in a ceremony at Camp Lejeune . \n</td>
    </tr>
    <tr>
      <th>8</th>
      <td>( i ) = Indoor \n</td>
    </tr>
    <tr>
      <th>9</th>
      <td></td>
    </tr>
  </tbody>
</table>


æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä¸€äº›æ–‡æœ¬æ˜¯ç»´åŸºç™¾ç§‘æ–‡ç« çš„å®Œæ•´æ®µè½ï¼Œè€Œå…¶ä»–çš„åªæ˜¯æ ‡é¢˜æˆ–ç©ºè¡Œã€‚

## å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelingï¼ŒCLMï¼‰

å¯¹äºå› æœè¯­è¨€æ¨¡å‹(CLM)ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬ï¼Œå¹¶åœ¨å®ƒä»¬è¢«åˆ†è¯åå°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†åœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å°†æ¥æ”¶å¦‚ä¸‹çš„è¿ç»­æ–‡æœ¬å—:

```
æ–‡æœ¬1
```
æˆ–
```
æ–‡æœ¬1ç»“å°¾ [BOS_TOKEN] æ–‡æœ¬2å¼€å¤´
```

å–å†³äºå®ƒä»¬æ˜¯å¦è·¨è¶Šæ•°æ®é›†ä¸­çš„å‡ ä¸ªåŸå§‹æ–‡æœ¬ã€‚æ ‡ç­¾å°†ä¸è¾“å…¥ç›¸åŒï¼Œä½†å‘å·¦ç§»åŠ¨ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[`distilgpt2`](https://huggingface.co/distilgpt2) æ¨¡å‹ã€‚æ‚¨åŒæ ·ä¹Ÿå¯ä»¥é€‰æ‹©[è¿™é‡Œ](https://huggingface.co/models?filter=causal-lm)åˆ—å‡ºçš„ä»»ä½•ä¸€ä¸ªcheckpoint:


```python
model_checkpoint = "distilgpt2"
```

ä¸ºäº†ç”¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„è¯æ±‡å¯¹æ‰€æœ‰æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œæˆ‘ä»¬å¿…é¡»ä¸‹è½½ä¸€ä¸ªé¢„å…ˆè®­ç»ƒè¿‡çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ã€‚è€Œè¿™äº›æ“ä½œéƒ½å¯ä»¥ç”±AutoTokenizerç±»å®Œæˆ:


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```

    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 762/762 [00:00<00:00, 358kB/s]
    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:04<00:00, 235kB/s]
    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:02<00:00, 217kB/s]
    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:05<00:00, 252kB/s]


æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ‰€æœ‰çš„æ–‡æœ¬è°ƒç”¨åˆ†è¯å™¨ï¼Œè¯¥æ“ä½œå¯ä»¥ç®€å•åœ°ä½¿ç”¨æ¥è‡ªDatasetsåº“çš„mapæ–¹æ³•å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåœ¨æ–‡æœ¬ä¸Šè°ƒç”¨æ ‡è®°å™¨çš„å‡½æ•°:


```python
def tokenize_function(examples):
    return tokenizer(examples["text"])
```

ç„¶åæˆ‘ä»¬å°†å®ƒåº”ç”¨åˆ°datasetså¯¹è±¡ä¸­çš„åˆ†è¯ï¼Œä½¿ç”¨```batch=True```å’Œ```4```ä¸ªè¿›ç¨‹æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚è€Œä¹‹åæˆ‘ä»¬å¹¶ä¸éœ€è¦```text```åˆ—ï¼Œæ‰€ä»¥å°†å…¶èˆå¼ƒã€‚



```python
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
```

    #0:   0%|          | 0/2 [00:00<?, ?ba/s]
    [A
    
    [A[A
    
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.42ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.87ba/s]
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.56ba/s]
    
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.73ba/s]
    #0:   0%|          | 0/10 [00:00<?, ?ba/s]
    [A
    
    #0:  10%|â–ˆ         | 1/10 [00:00<00:03,  2.87ba/s]
    [A
    
    #0:  20%|â–ˆâ–ˆ        | 2/10 [00:00<00:02,  2.89ba/s]
    [A
    
    #0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:02,  3.08ba/s]
    [A
    
    #0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:01<00:01,  3.14ba/s]
    [A
    
    #0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:01<00:01,  3.33ba/s]
    [A
    
    #0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:01<00:01,  3.44ba/s]
    [A
    
    [A[A
    #0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:02<00:01,  2.89ba/s]
    
    [A[A
    #0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:02<00:00,  2.89ba/s]
    
    #0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:02<00:00,  3.04ba/s]
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.37ba/s]
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.44ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  3.33ba/s]
    
    
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.25ba/s]
    #0:   0%|          | 0/1 [00:00<?, ?ba/s]
    [A
    
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.70ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.79ba/s]
    
    [A
    
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.74ba/s]
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.82ba/s]


å¦‚æœæˆ‘ä»¬ç°åœ¨æŸ¥çœ‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æ–‡æœ¬å·²ç»è¢«æ¨¡å‹æ‰€éœ€çš„input_idsæ‰€å–ä»£:


```python
tokenized_datasets["train"][1]
```




    {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],
     'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]}



ä¸‹ä¸€æ­¥å°±æœ‰ç‚¹å°å›°éš¾äº†ï¼šæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åå°†ç»“æœåˆ†å‰²æˆç‰¹å®š`block_size`çš„å°å—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨`map`æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨é€‰é¡¹`batch=True`ã€‚è¿™ä¸ªé€‰é¡¹å…è®¸æˆ‘ä»¬é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è·å–é¢„è®­ç»ƒæ¨¡å‹æ—¶æ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚æœ€å¤§é•¿åº¦åœ¨è¿™é‡Œè®¾ç½®ä¸º128ï¼Œä»¥é˜²æ‚¨çš„æ˜¾å­˜çˆ†ç‚¸ğŸ’¥ã€‚


```python
# block_size = tokenizer.model_max_length
block_size = 128
```

ç„¶åæˆ‘ä»¬ç¼–å†™é¢„å¤„ç†å‡½æ•°æ¥å¯¹æˆ‘ä»¬çš„æ–‡æœ¬è¿›è¡Œåˆ†ç»„:


```python
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚
    total_length = (total_length // block_size) * block_size
    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```

é¦–å…ˆæ³¨æ„ï¼Œæˆ‘ä»¬å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ã€‚

è¿™æ˜¯å› ä¸ºğŸ¤—transformeråº“çš„æ¨¡å‹é»˜è®¤å‘å³ç§»åŠ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨æ“ä½œã€‚

è¿˜è¦æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œ`map`æ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆ é™¤å‰©ä½™éƒ¨åˆ†ï¼Œä½¿è¿æ¥çš„æ ‡è®°åŒ–æ–‡æœ¬æ¯1000ä¸ªç¤ºä¾‹ä¸º`block_size`çš„å€æ•°ã€‚æ‚¨å¯ä»¥é€šè¿‡ä¼ é€’æ›´é«˜çš„æ‰¹å¤„ç†å¤§å°æ¥è°ƒæ•´æ­¤è¡Œä¸º(å½“ç„¶è¿™ä¹Ÿä¼šè¢«å¤„ç†å¾—æ›´æ…¢)ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`multiprocessing`æ¥åŠ é€Ÿé¢„å¤„ç†:


```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```

    #0:   0%|          | 0/2 [00:00<?, ?ba/s]
    [A
    
    [A[A
    
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.12ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.89ba/s]
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.60ba/s]
    
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.94ba/s]
    #0:   0%|          | 0/10 [00:00<?, ?ba/s]
    [A
    
    #0:  10%|â–ˆ         | 1/10 [00:00<00:03,  2.90ba/s]
    [A
    
    #0:  20%|â–ˆâ–ˆ        | 2/10 [00:00<00:02,  2.76ba/s]
    [A
    
    #0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:01<00:02,  2.72ba/s]
    [A
    
    #0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:01<00:02,  2.75ba/s]
    [A
    
    #0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:01<00:01,  2.92ba/s]
    #0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:02<00:01,  3.01ba/s]
    
    [A[A
    [A
    #0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:02<00:01,  2.69ba/s]
    
    #0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:02<00:00,  2.67ba/s]
    [A
    
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.00ba/s]
    
    
    [A[A
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  3.04ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.88ba/s]
    
    
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.79ba/s]
    #0:   0%|          | 0/1 [00:00<?, ?ba/s]
    [A
    
    #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.41ba/s]
    #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.61ba/s]
    
    
    #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.69ba/s]
    
    #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.55ba/s]


ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ï¼šç°åœ¨æ ·æœ¬åŒ…å«äº†`block_size`è¿ç»­å­—ç¬¦å—ï¼Œå¯èƒ½è·¨è¶Šäº†å‡ ä¸ªåŸå§‹æ–‡æœ¬ã€‚


```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```




    ' game and follows the " Nameless ", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " Calamaty Raven ". \n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'



æ—¢ç„¶æ•°æ®å·²ç»æ¸…ç†å®Œæ¯•ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®ä¾‹åŒ–æˆ‘ä»¬çš„è®­ç»ƒå™¨äº†ã€‚æˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªæ¨¡å‹:


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
```

    Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 353M/353M [00:21<00:00, 16.0MB/s]


æ£€æŸ¥torchç‰ˆæœ¬


```python

import importlib.util
# import importlib_metadata
a = importlib.util.find_spec("torch") is not None
print(a)
# _torch_version = importlib_metadata.version("torch")
# print(_torch_version)
```

    True


å’Œä¸€äº›`TrainingArguments`:


```python
from transformers import Trainer, TrainingArguments
```


```python
training_args = TrainingArguments(
    "test-clm",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)
```

æˆ‘ä»¬æŠŠè¿™äº›éƒ½ä¼ é€’ç»™`Trainer`ç±»:


```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"][:1000],
    eval_dataset=lm_datasets["validation"][:1000],
)
```

ç„¶åå°±å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ğŸŒ¶:


```python
trainer.train()
```

      0%|          | 0/3 [00:00<?, ?it/s]


    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    /var/folders/2k/x3py0v857kgcwqvvl00xxhxw0000gn/T/ipykernel_12460/4032920361.py in <module>
    ----> 1 trainer.train()
    

    ~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
       1032             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)
       1033 
    -> 1034             for step, inputs in enumerate(epoch_iterator):
       1035 
       1036                 # Skip past any already trained steps if resuming training


    ~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py in __next__(self)
        519             if self._sampler_iter is None:
        520                 self._reset()
    --> 521             data = self._next_data()
        522             self._num_yielded += 1
        523             if self._dataset_kind == _DatasetKind.Iterable and \


    ~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py in _next_data(self)
        559     def _next_data(self):
        560         index = self._next_index()  # may raise StopIteration
    --> 561         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
        562         if self._pin_memory:
        563             data = _utils.pin_memory.pin_memory(data)


    ~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
         42     def fetch(self, possibly_batched_index):
         43         if self.auto_collation:
    ---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
         45         else:
         46             data = self.dataset[possibly_batched_index]


    ~/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py in <listcomp>(.0)
         42     def fetch(self, possibly_batched_index):
         43         if self.auto_collation:
    ---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
         45         else:
         46             data = self.dataset[possibly_batched_index]


    KeyError: 1


ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¾—åˆ°å®ƒåœ¨éªŒè¯é›†ä¸Šçš„perplexityï¼Œå¦‚ä¸‹æ‰€ç¤º:


```python
import math
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

## æ©è”½è¯­è¨€æ¨¡å‹ï¼ˆMask Language Modelingï¼ŒMLMï¼‰

æ©è”½è¯­è¨€æ¨¡å‹(MLM)æˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†é¢„å¤„ç†å’Œä»¥å‰ä¸€æ ·ç”¨ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ï¼š

æˆ‘ä»¬å°†éšæœº"MASK"ä¸€äº›å­—ç¬¦(ä½¿ç”¨"[MASK]"è¿›è¡Œæ›¿æ¢)ä»¥åŠè°ƒæ•´æ ‡ç­¾ä¸ºåªåŒ…å«åœ¨"[MASK]"ä½ç½®å¤„çš„æ ‡ç­¾(å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦é¢„æµ‹æ²¡æœ‰è¢«"MASK"çš„å­—ç¬¦)ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[`distilroberta-base`](https://huggingface.co/distilroberta-base)æ¨¡å‹ã€‚æ‚¨åŒæ ·ä¹Ÿå¯ä»¥é€‰æ‹©[è¿™é‡Œ](https://huggingface.co/models?filter=causal-lm)åˆ—å‡ºçš„ä»»ä½•ä¸€ä¸ªcheckpoint:


```python
model_checkpoint = "distilroberta-base"
```

æˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·åº”ç”¨ç›¸åŒçš„åˆ†è¯å™¨å‡½æ•°ï¼Œæˆ‘ä»¬åªéœ€è¦æ›´æ–°æˆ‘ä»¬çš„åˆ†è¯å™¨æ¥ä½¿ç”¨åˆšåˆšé€‰æ‹©çš„checkpoint:


```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
```

åƒä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬æŠŠæ–‡æœ¬åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå¹¶æŠŠå®ƒä»¬åˆ†æˆé•¿åº¦ä¸º`block_size`çš„æ ·æœ¬ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ç”±å•ç‹¬çš„å¥å­ç»„æˆï¼Œåˆ™å¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ã€‚


```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```

å‰©ä¸‹çš„å’Œæˆ‘ä»¬ä¹‹å‰çš„åšæ³•éå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ä¸¤ä¸ªä¾‹å¤–ã€‚é¦–å…ˆæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€‚åˆæ©è”½è¯­è¨€æ¨¡å‹çš„æ¨¡å‹:


```python
from transformers import AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„data_collatorã€‚data_collatoræ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä»€ä¹ˆç‰¹æ®Šçš„äº‹æƒ…è¦åšï¼Œæ‰€ä»¥æˆ‘ä»¬åªä½¿ç”¨è¿™ä¸ªå‚æ•°çš„é»˜è®¤å€¼ã€‚è¿™é‡Œæˆ‘ä»¬è¦åšéšæœº"MASK"ã€‚

æˆ‘ä»¬å¯ä»¥å°†å…¶ä½œä¸ºé¢„å¤„ç†æ­¥éª¤(`tokenizer`)è¿›è¡Œå¤„ç†ï¼Œä½†åœ¨æ¯ä¸ªé˜¶æ®µï¼Œå­—ç¬¦æ€»æ˜¯ä»¥ç›¸åŒçš„æ–¹å¼è¢«æ©ç›–ã€‚é€šè¿‡åœ¨data_collatorä¸­æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ¯æ¬¡æ£€æŸ¥æ•°æ®æ—¶éƒ½ä»¥æ–°çš„æ–¹å¼å®Œæˆéšæœºæ©è”½ã€‚

ä¸ºäº†å®ç°æ©è”½ï¼Œ`Transformers`ä¸ºæ©è”½è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ª`DataCollatorForLanguageModeling`ã€‚æˆ‘ä»¬å¯ä»¥è°ƒæ•´æ©è”½çš„æ¦‚ç‡:


```python
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ç„¶åæˆ‘ä»¬è¦æŠŠæ‰€æœ‰çš„ä¸œè¥¿äº¤ç»™trainerï¼Œç„¶åå¼€å§‹è®­ç»ƒ:


```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"][:1000],
    eval_dataset=lm_datasets["validation"][:100],
    data_collator=data_collator,
)
```


```python
trainer.train()
```

åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

ä¸CLMç›®æ ‡ç›¸æ¯”ï¼Œå›°æƒ‘åº¦è¦ä½å¾—å¤šï¼Œå› ä¸ºå¯¹äºMLMç›®æ ‡ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹éšè—çš„ä»¤ç‰Œ(åœ¨è¿™é‡Œå æ€»æ•°çš„15%)è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶å¯ä»¥è®¿é—®å…¶ä½™çš„ä»¤ç‰Œã€‚

å› æ­¤ï¼Œå¯¹äºæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€é¡¹æ›´å®¹æ˜“çš„ä»»åŠ¡ã€‚


```python
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```


```python
ä¸è¦å¿˜è®°å°†ä½ çš„æ¨¡å‹[ä¸Šä¼ ](https://huggingface.co/transformers/model_sharing.html)åˆ°[ğŸ¤— æ¨¡å‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚
```
